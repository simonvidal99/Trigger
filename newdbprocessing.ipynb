{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "# %matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from itertools import product\n",
    "from itertools import groupby\n",
    "import itertools\n",
    "import pickle\n",
    "import glob\n",
    "import chardet\n",
    "import copy\n",
    "from natsort import natsorted\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Set a professional style for the plot\n",
    "plt.style.use('_mpl-gallery')\n",
    "import matplotlib.dates as mdaates\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.cm as cm\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset, plot_trigger\n",
    "from obspy import Trace, Stream\n",
    "from obspy.imaging.spectrogram import spectrogram\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from tqdm.auto import tqdm\n",
    "from icecream import ic\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm, kstest, skew, kurtosis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Local Imports\n",
    "from energy.utils_general import *\n",
    "from energy.utils_energy import *\n",
    "from energy.preprocessing import *\n",
    "from energy.metrics import *\n",
    "from picking.p_picking import p_picking_each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tener en consideración. Las rutas a los archivos están guardadas de la siguiente forma: *waveform_M/event_ID/channel/station/eventid_canal_horainicio_horatermino.mseed*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creamos un diccionario con las coordenadas de las estaciones del litoral a partir del archivo Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "v_P = 8.064\n",
    "\n",
    "file_stations_coord = os.path.join(\"BD paper\", \"catalogos\",\"Estaciones_Chile.csv\")\n",
    "df = pd.read_csv(file_stations_coord, sep=';')\n",
    "stations_coord_all = df.set_index('Station')[['Latitude', 'Longitude']].apply(tuple, axis=1).to_dict()\n",
    "stations_names = list(stations_coord_all.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Trabajar con la base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. La función z_channel_dict entrega un nested dictionary donde la primera capa de llaves indica la magnitud, la segunda capa el evento, la tercera la red de la estación que la captó, y finalmente el valor es la traza ya leída por obspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# Get the path to all the folders from the folder BD paper that is in the current directory\n",
    "#folders = glob.glob('BD paper/*')\n",
    "folders = glob.glob(os.path.join('BD paper', '*'))\n",
    "# Get all the folders in the BD paper folder that start with waveform \n",
    "#folders_signals = [folder for folder in folders if folder.startswith('BD paper/waveform')]\n",
    "folders_signals = [folder for folder in folders if folder.startswith(os.path.join('BD paper', 'waveform'))]\n",
    "\n",
    "folders_signals = sorted(folders_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "folders_signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función se puede/debe optimizar. Usando os.walk probablemente sale más rápido. No sé si existe alguna forma de hacerla más generalizable eso si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "source": [
    "inventory_path = \"inventory\"\n",
    "def z_channel_dict(folders_signals, inventory_path=inventory_path):\n",
    "\n",
    "    result_dict_z = OrderedDict()\n",
    "    #result_dict_e = OrderedDict()\n",
    "    #result_dict_n = OrderedDict()\n",
    "\n",
    "\n",
    "    for folder in folders_signals:\n",
    "        waveform_type = os.path.split(folder)[-1]\n",
    "        events_id = natsorted(glob.glob(os.path.join(folder, '*')))\n",
    "\n",
    "        for event_id in events_id:\n",
    "            events_name = os.path.split(event_id)[-1]\n",
    "            network_path = glob.glob(os.path.join(event_id, '*'))\n",
    "\n",
    "            for network_folder in network_path:\n",
    "                network_name = os.path.split(network_folder)[-1]\n",
    "                \n",
    "                stations_path = glob.glob(os.path.join(network_folder, '*'))\n",
    "\n",
    "                for station_path in stations_path:\n",
    "                    stations_names = os.path.split(station_path)[-1]\n",
    "                    #ic(stations_names)\n",
    "                    stations_path = glob.glob(os.path.join(station_path, '*'))\n",
    "                    #ic(stations_path)\n",
    "\n",
    "                    stations_ch_z = [station_z for station_z in stations_path if 'BHZ' in station_z]\n",
    "                    stations_ch_e = [station_e for station_e in stations_path if 'BHE' in station_e]\n",
    "                    stations_ch_n = [station_n for station_n in stations_path if 'BHN' in station_n]\n",
    "                    \n",
    "                    traces = []\n",
    "\n",
    "                    #ic(len(stations_ch_z))\n",
    "                    # Read and store traces using ObsPy\n",
    "                    # traces_z = []\n",
    "\n",
    "                    # Files to remove response\n",
    "                    file_response = glob.glob(os.path.join(inventory_path, f\"*{stations_names}.xml\"))\n",
    "                    # #ic(file_response)\n",
    "\n",
    "\n",
    "                    # trace_z = read(stations_ch_z[0])[0]  # Assuming one trace per file\n",
    "                    \n",
    "                    if file_response:\n",
    "                        remove_file_z = file_response[0]\n",
    "\n",
    "                    # #ic(remove_file_z)\n",
    "                    # trace_resp_z = trace_z.copy()\n",
    "                    # trace_removed_z = remove_response(trace_resp_z, remove_file_z , 'obspy')\n",
    "                    # #st_resp[2] = st_removed\n",
    "                    # st_z = trace_removed_z.copy()\n",
    "                    # st_z.filter('bandpass', freqmin=4.0, freqmax=10.0)\n",
    "                    \n",
    "                    # traces_z.append(st_z)\n",
    "\n",
    "\n",
    "\n",
    "                    trace_Z = read(stations_ch_z[0])[0]\n",
    "                    trace_resp = trace_Z.copy()\n",
    "                    trace_resp = remove_response(trace_resp, remove_file_z , 'obspy')\n",
    "                    trace = Stream(traces = trace_resp)\n",
    "                    if len(stations_ch_e) > 0:\n",
    "                        trace += read(stations_ch_e[0])[0]\n",
    "                    if len(stations_ch_n) > 0:    \n",
    "                        trace += read(stations_ch_n[0])[0]\n",
    "                    \n",
    "\n",
    "                    # if file_response:\n",
    "                    #     remove_file = file_response[0]\n",
    "\n",
    "                    # ic(remove_file)\n",
    "\n",
    "                    # tr_resp = trace.copy()\n",
    "                    # for i in range(len(trace)):\n",
    "                    #     st_removed = remove_response(tr_resp.select(channel=tr_resp[i].stats.channel)[0], remove_file , 'obspy')\n",
    "                    #     tr_resp[i] = st_removed\n",
    "                    #     ic(i)\n",
    "\n",
    "                    tr_filtered = trace.copy()\n",
    "                    tr_filtered.filter('bandpass', freqmin=4.0, freqmax=10.0)\n",
    "                    traces.append(tr_filtered)\n",
    "\n",
    "                    result_dict_z.setdefault(waveform_type, {}).setdefault(events_name, {}).setdefault(network_name, {}).setdefault(stations_names, traces)\n",
    "\n",
    "\n",
    "    return result_dict_z\n",
    "\n",
    "event_dict = z_channel_dict(folders_signals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "source": [
    "inventory_path = \"inventory\"\n",
    "def no_filer_dict(folders_signals, inventory_path=inventory_path):\n",
    "\n",
    "    no_filter = OrderedDict()\n",
    "    #result_dict_e = OrderedDict()\n",
    "    #result_dict_n = OrderedDict()\n",
    "\n",
    "\n",
    "    for folder in folders_signals:\n",
    "        waveform_type = os.path.split(folder)[-1]\n",
    "        events_id = natsorted(glob.glob(os.path.join(folder, '*')))\n",
    "\n",
    "        for event_id in events_id:\n",
    "            events_name = os.path.split(event_id)[-1]\n",
    "            network_path = glob.glob(os.path.join(event_id, '*'))\n",
    "\n",
    "            for network_folder in network_path:\n",
    "                network_name = os.path.split(network_folder)[-1]\n",
    "                \n",
    "                stations_path = glob.glob(os.path.join(network_folder, '*'))\n",
    "\n",
    "                for station_path in stations_path:\n",
    "                    stations_names = os.path.split(station_path)[-1]\n",
    "                    #ic(stations_names)\n",
    "                    stations_path = glob.glob(os.path.join(station_path, '*'))\n",
    "                    #ic(stations_path)\n",
    "\n",
    "                    stations_ch_z = [station_z for station_z in stations_path if 'BHZ' in station_z]\n",
    "                    stations_ch_e = [station_e for station_e in stations_path if 'BHE' in station_e]\n",
    "                    stations_ch_n = [station_n for station_n in stations_path if 'BHN' in station_n]\n",
    "                    \n",
    "                    traces = []\n",
    "\n",
    "                    #ic(len(stations_ch_z))\n",
    "                    # Read and store traces using ObsPy\n",
    "                    # traces_z = []\n",
    "\n",
    "                    # Files to remove response\n",
    "                    file_response = glob.glob(os.path.join(inventory_path, f\"*{stations_names}.xml\"))\n",
    "                    # #ic(file_response)\n",
    "\n",
    "\n",
    "                    # trace_z = read(stations_ch_z[0])[0]  # Assuming one trace per file\n",
    "                    \n",
    "                    if file_response:\n",
    "                        remove_file_z = file_response[0]\n",
    "\n",
    "                    # #ic(remove_file_z)\n",
    "                    # trace_resp_z = trace_z.copy()\n",
    "                    # trace_removed_z = remove_response(trace_resp_z, remove_file_z , 'obspy')\n",
    "                    # #st_resp[2] = st_removed\n",
    "                    # st_z = trace_removed_z.copy()\n",
    "                    # st_z.filter('bandpass', freqmin=4.0, freqmax=10.0)\n",
    "                    \n",
    "                    # traces_z.append(st_z)\n",
    "\n",
    "\n",
    "\n",
    "                    trace_Z = read(stations_ch_z[0])[0]\n",
    "                    trace_resp = trace_Z.copy()\n",
    "                    #trace_resp = remove_response(trace_resp, remove_file_z , 'obspy')\n",
    "                    trace = Stream(traces = trace_resp)\n",
    "                    if len(stations_ch_e) > 0:\n",
    "                        trace += read(stations_ch_e[0])[0]\n",
    "                    if len(stations_ch_n) > 0:    \n",
    "                        trace += read(stations_ch_n[0])[0]\n",
    "                    \n",
    "\n",
    "                    # if file_response:\n",
    "                    #     remove_file = file_response[0]\n",
    "\n",
    "                    # ic(remove_file)\n",
    "\n",
    "                    # tr_resp = trace.copy()\n",
    "                    # for i in range(len(trace)):\n",
    "                    #     st_removed = remove_response(tr_resp.select(channel=tr_resp[i].stats.channel)[0], remove_file , 'obspy')\n",
    "                    #     tr_resp[i] = st_removed\n",
    "                    #     ic(i)\n",
    "\n",
    "                    tr_filtered = trace.copy()\n",
    "                    #tr_filtered.filter('bandpass', freqmin=4.0, freqmax=10.0)\n",
    "                    traces.append(tr_filtered)\n",
    "\n",
    "                    no_filter.setdefault(waveform_type, {}).setdefault(events_name, {}).setdefault(network_name, {}).setdefault(stations_names, traces)\n",
    "\n",
    "\n",
    "    return no_filter\n",
    "\n",
    "event_dict_no_filter = no_filer_dict(folders_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "waveform_type = list(event_dict.keys())\n",
    "\n",
    "# get all the events ids consideting waveform type is a list. And make events_ids a dictionary with the waveform type as keys\n",
    "events_ids = {waveform_type: [int(event_id) for event_id in event_dict[waveform_type].keys()] for waveform_type in waveform_type}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. La siguiente función pone a todas las traces de TODOS los eventos en una fila, esto lo tengo a priori, quizás habrá que arreglarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "def get_all_traces(nested_dict):\n",
    "    traces = []\n",
    "    for value in nested_dict.values():\n",
    "        if isinstance(value, dict):\n",
    "            traces.extend(get_all_traces(value))\n",
    "        else:\n",
    "            traces.extend(value)\n",
    "    return traces\n",
    "\n",
    "# Get all ObsPy Trace objects in the dictionary\n",
    "all_traces = get_all_traces(event_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 El siguiente código es para crear un nuevo archivo CSV que contenga solo los siguientes datos:\n",
    "- EventId\n",
    "- Time\n",
    "- Latitud\n",
    "- Longitud\n",
    "- Magnitud\n",
    "- Estación (la que aparece automáticamente en el catálogo)\n",
    "- Estaciones más cercanas (calculada con geodisic. Cantidad arbitraria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# FUNCION QUE USABA ANTES CUANDO TRABAJA CON CARPETA LISTA DE EVENTOS. PODRÍA SERVIR EN UN FUTURO\n",
    "\n",
    "'''\n",
    "def merge_coordinates(files_to_add, files_with_coord):\n",
    "    \n",
    "    for file_to_add, file_with_coords in zip(files_to_add, files_with_coord):\n",
    "        df_to_add = pd.read_csv(file_to_add, sep=',')\n",
    "        df_with_coords = pd.read_csv(file_with_coords, sep='|')\n",
    "\n",
    "        df_to_add['Time'] = pd.to_datetime(df_to_add['Time'])\n",
    "        df_with_coords['Time'] = pd.to_datetime(df_with_coords['Time'])\n",
    "\n",
    "        df_to_add = df_to_add.set_index('Time')\n",
    "\n",
    "        # Merge df_to_add with df_with_coords on 'Time', keeping only 'Latitude' and 'Longitude' from df_with_coords\n",
    "        df_to_add = pd.merge(df_to_add, df_with_coords[['Time','Latitude', 'Longitude']], on='Time', how='left', suffixes=('', '_y'))\n",
    "\n",
    "        # Drop the '_y' columnsbb\n",
    "        df_to_add = df_to_add.drop(columns=['Latitude_y', 'Longitude_y'], errors='ignore')\n",
    "\n",
    "        df_to_add.to_csv(file_to_add, index=False, sep = ',')\n",
    "\n",
    "\n",
    "# Get all the csv files in the BD paper folder that have the word \"cercanos\" in their name\n",
    "files_raw = sorted(glob.glob(os.path.join('BD paper', 'catalogos','*cercanos*.csv')))\n",
    "# Get all the csv files in the BD paper folder that have the word \"full_data\" in their name\n",
    "files_full_data = sorted(glob.glob(os.path.join('BD paper', 'lista de datos','*full_data*.csv')))\n",
    "\n",
    "\n",
    "# Call the function to merge coordinates for each pair of files\n",
    "merge_coordinates(files_raw, files_full_data)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "# Get all the csv files in the BD paper folder that have the word \"cercanos\" in their name\n",
    "files_raw = sorted(glob.glob(os.path.join('BD paper', 'catalogos','*descargados*.csv')))\n",
    "\n",
    "files_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "def filter_csv(files_raw, events_ids):\n",
    "    # Columnas a mantener\n",
    "    columns_to_keep = [\"#EventID\", \"Time\", \"Latitude\", \"Longitude\", \"Magnitude\", \"Estacion\"]\n",
    "\n",
    "    for i, (waveform_type, event_ids) in enumerate(events_ids.items()):\n",
    "        # Leer el archivo CSV\n",
    "        df = pd.read_csv(files_raw[i], sep=';')\n",
    "\n",
    "        # Filtrar las filas donde '#EventID' está en event_ids\n",
    "        df = df[df['#EventID'].isin(event_ids)]\n",
    "\n",
    "        # Ordernar la columna de #EventID\n",
    "        df = df.sort_values(by='#EventID')\n",
    "\n",
    "        # Mantener solo las columnas deseadas\n",
    "        df = df[columns_to_keep]\n",
    "\n",
    "        # Guardar el nuevo DataFrame en un nuevo archivo CSV\n",
    "        df.to_excel(os.path.join(\"BD paper\", \"catalogos\", f\"{waveform_type}_filtered.xlsx\"), index=False)\n",
    "\n",
    "filter_csv(files_raw, events_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Ahora que ya se tienen archivos .csv con columnas \"Event\"Time\", \"Magnitud\", \"Estación\", \"Latitud\", \"Longitud\", podemos verificar efectivamente cuál es la estación más cercana y agregarla como última columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "files_filtered = sorted(glob.glob(os.path.join('BD paper', 'catalogos','*filtered*')))\n",
    "files_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función además agrega columnas con el tiempo estimado donde el sismo debería verse en la i-ésima estación más cercana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "def closest_station(file_path, stations_coord_all, v_P, n_closest_stations = 7):\n",
    "    '''\n",
    "    Actualiza todos los archivos csv con la estación más cercana a cada evento sísmico.\n",
    "    '''\n",
    "\n",
    "    for file in file_path:\n",
    "        ic(file)\n",
    "\n",
    "        df_events = pd.read_excel(file)\n",
    "\n",
    "        # Cambiar nombre de la columna \"Time\" a \"Fecha UTC\" y cosas en inglés por español\n",
    "        df_events = df_events.rename(columns={'Time': 'Fecha UTC', 'Latitude': 'Latitud', 'Longitude': 'Longitud', 'Magnitude': 'Magnitud'})\n",
    "        \n",
    "        # Cambiar esta columna a formato UTC\n",
    "        df_events['Fecha UTC'] = pd.to_datetime(df_events['Fecha UTC'])\n",
    "        #df_events = df_events.rename(columns={'Latitude': 'Latitud', 'Longitude': 'Longitud'})\n",
    "\n",
    "        df_events.to_excel(file, index=False)\n",
    "\n",
    "        # Tomar el valor máximo y mínimo de la magnitud en el dataframe\n",
    "        max_magnitude = df_events['Magnitud'].max()\n",
    "        min_magnitude = df_events['Magnitud'].min()\n",
    "\n",
    "        df_new = calculate_detection_times(df_events, stations_coord_all, v_P, magnitude_range=(min_magnitude, max_magnitude))\n",
    "        _, closest_sts_names = nearest_n_stations(df_new, stations_names, n_closest_stations)\n",
    "\n",
    "        df_events = pd.read_excel(file)\n",
    "\n",
    "        # Agregar columnas para cada estación cercana\n",
    "        for i in range(n_closest_stations):\n",
    "            col_name = f'Estación más cercana {i+1}'\n",
    "            df_events[col_name] = closest_sts_names[i]\n",
    "\n",
    "        df_events.to_excel(file, index=False)\n",
    "\n",
    "        df_events = pd.read_excel(file)\n",
    "\n",
    "        # Agregar columnas para el tiempo de inicio de cada estación cercana\n",
    "        for i in range(n_closest_stations):\n",
    "            col_name = f'Inicio estación más cercana {i+1}'\n",
    "            df_events[col_name] = df_events.apply(lambda row: df_new.loc[row.name, f'Inicio_{row[f\"Estación más cercana {i+1}\"]}'], axis=1)\n",
    "\n",
    "        # Escribir en el archivo excel\n",
    "        df_events.to_excel(file, index=False)\n",
    "\n",
    "\n",
    "closest_station(files_filtered, stations_coord_all, v_P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "source": [
    "event_dict['waveform_6-9']['5159025']['C1']['CO03'][0][0].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "source": [
    "event_dict_no_filter['waveform_6-9']['5159025']['C1']['CO03'][0][0].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Procesamiento de las trazas\n",
    "Ahora que ya tenemos los archivos excel con las estaciones más cercanas y con sus estimados tiempos de detección del evento, podemos empezar a trabajar con las trazas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Calculo de la potencia\n",
    "Se deben juntar todas las trazas que pertenecen a cada intervalo de magnitud en una sola lista para luego calcular la potencia. Es decir, todas las que son entre 4 y 5, todas las que son entre 5 y 6, y todas las que son mayores que 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "\n",
    "'''\n",
    "def find_matching_trace(event_dict, dataframe, waveform_key, station_columns = list_closest_stations):\n",
    "    trace_list = []\n",
    "    event_dict_waveform = event_dict[waveform_key]\n",
    "    #ic(event_dict_waveform)\n",
    "\n",
    "    for i in range(len(dataframe['#EventID'].astype(str))):\n",
    "        channels = list(event_dict_waveform[dataframe['#EventID'].astype(str).iloc[i]].keys())\n",
    "        #station_data = list(event_dict_waveform[dataframe['#EventID'].astype(str).iloc[i]].values())\n",
    "        #station_names = [key for station_dict in station_data for key in station_dict.keys()]\n",
    "\n",
    "        station_data = {key: value for sublist in list(event_dict_waveform[dataframe['#EventID'].astype(str).iloc[i]].values()) for key, value in sublist.items()}\n",
    "        #ic(station_data)\n",
    "        station_names = list(station_data.keys())\n",
    "    \n",
    "\n",
    "        for column in dataframe.loc[:, station_columns]:\n",
    "            station_column_value = dataframe[column].astype(str).iloc[i]\n",
    "\n",
    "            matching_station = next((station for station in station_names if station in station_column_value), None)\n",
    "\n",
    "            if matching_station is not None:\n",
    "                #trace_list.append(station_data[0][matching_station])\n",
    "                #ic(matching_station)\n",
    "                #ic(station_data)\n",
    "                #ic(station_data[matching_station][0])\n",
    "\n",
    "                trace_list.extend(station_data[matching_station])\n",
    "                break\n",
    "            else:\n",
    "                # Tu código si no hay ninguna coincidencia\n",
    "                pass\n",
    "\n",
    "    return trace_list\n",
    "\n",
    "\n",
    "df_mag4p1 = pd.read_excel(files_filtered[0])\n",
    "df_mag4p2 = pd.read_excel(files_filtered[1])\n",
    "df_mag5 = pd.read_excel(files_filtered[2])\n",
    "df_mag69 = pd.read_excel(files_filtered[3])\n",
    "\n",
    "\n",
    "trace_mag4p1 = find_matching_trace(event_dict, df_mag4p1, waveform_type[0], list_closest_stations)\n",
    "#trace_mag4p2 = find_matching_trace(event_dict, df_mag4p2, waveform_type[1], list_closest_stations)\n",
    "#trace_mag5 = find_matching_trace(event_dict, df_mag5, waveform_type[2], list_closest_stations)\n",
    "#trace_mag69 = find_matching_trace(event_dict, df_mag69, waveform_type[3], list_closest_stations)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def find_matching_trace_v2(event_dict, dataframe, waveform_key, station_columns = list_closest_stations):\n",
    "#     trace_list = []\n",
    "#     event_dict_waveform = event_dict[waveform_key]\n",
    "\n",
    "#     for i in range(len(dataframe['#EventID'].astype(str))):\n",
    "#         #channels = list(event_dict_waveform[dataframe['#EventID'].astype(str).iloc[i]].keys())\n",
    "#         station_data = {key: value for sublist in list(event_dict_waveform[dataframe['#EventID'].astype(str).iloc[i]].values()) for key, value in sublist.items()}\n",
    "#         #ic(station_data)\n",
    "#         station_names = list(station_data.keys())\n",
    "        \n",
    "\n",
    "#         for j, column in enumerate(dataframe.loc[:, station_columns]):\n",
    "#             station_column_value = dataframe[column].astype(str).iloc[i]\n",
    "#             matching_station = next((station for station in station_names if station in station_column_value), None)\n",
    "\n",
    "#             if matching_station is not None:\n",
    "#                 # Cambiar el tiempo de inicio de la traza\n",
    "#                 start_time_column = f\"Inicio estación más cercana {j+1}\"\n",
    "#                 start_time = UTCDateTime(dataframe[start_time_column].iloc[i])\n",
    "#                 #ic(start_time)\n",
    "                \n",
    "#                 trace = station_data[matching_station][0]\n",
    "#                 #ic(trace)\n",
    "#                 trace[0].stats.starttime = start_time\n",
    "\n",
    "#                 trace_list.extend(station_data[matching_station])\n",
    "#                 break\n",
    "\n",
    "#     return trace_list\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "source": [
    "list_closest_stations = ['Estación más cercana 1', 'Estación más cercana 2', 'Estación más cercana 3', 'Estación más cercana 4', 'Estación más cercana 5', 'Estación más cercana 6', 'Estación más cercana 7']\n",
    "\n",
    "\n",
    "def find_matching_trace_v2(event_dict, dataframe, waveform_key, station_columns=list_closest_stations):\n",
    "    trace_dict = {}  # Cambié trace_list por trace_dict\n",
    "\n",
    "    event_dict_waveform = event_dict[waveform_key]\n",
    "\n",
    "    for i in range(len(dataframe['#EventID'].astype(str))):\n",
    "        station_data = {key: value for sublist in\n",
    "                        list(event_dict_waveform[dataframe['#EventID'].astype(str).iloc[i]].values()) for key, value\n",
    "                        in sublist.items()}\n",
    "        station_names = list(station_data.keys())\n",
    "\n",
    "        for j, column in enumerate(dataframe.loc[:, station_columns]):\n",
    "            station_column_value = dataframe[column].astype(str).iloc[i]\n",
    "            matching_station = next((station for station in station_names if station in station_column_value), None)\n",
    "\n",
    "            if matching_station is not None:\n",
    "                start_time_column = f\"Inicio estación más cercana {j + 1}\"\n",
    "                start_time = UTCDateTime(dataframe[start_time_column].iloc[i])\n",
    "\n",
    "                trace = station_data[matching_station][0]\n",
    "                trace[0].stats.starttime = start_time\n",
    "\n",
    "                # Cambiado trace_list por trace_dict\n",
    "                if dataframe['#EventID'].iloc[i] not in trace_dict:\n",
    "                    trace_dict[dataframe['#EventID'].iloc[i]] = []\n",
    "                trace_dict[dataframe['#EventID'].iloc[i]].extend(station_data[matching_station])\n",
    "\n",
    "                break\n",
    "\n",
    "    return trace_dict\n",
    "\n",
    "df_mag12 = pd.read_excel(files_filtered[0])\n",
    "df_mag3p1 = pd.read_excel(files_filtered[1])\n",
    "df_mag3p2 = pd.read_excel(files_filtered[2])\n",
    "df_mag4p1 = pd.read_excel(files_filtered[3])\n",
    "df_mag4p2 = pd.read_excel(files_filtered[4])\n",
    "df_mag5 = pd.read_excel(files_filtered[5])\n",
    "df_mag69 = pd.read_excel(files_filtered[6])\n",
    "\n",
    "def create_traces(event_dict, dataframe, waveform_key, station_columns):\n",
    "    trace_dict = find_matching_trace_v2(event_dict, dataframe, waveform_key, station_columns)\n",
    "    trace_Z = {event_id: trace[0].select(channel='BHZ')[0] for event_id, trace in trace_dict.items()}\n",
    "    trace_E = {event_id: trace[0].select(channel='BHE')[0] for event_id, trace in trace_dict.items()}\n",
    "    trace_N = {event_id: trace[0].select(channel='BHN')[0] for event_id, trace in trace_dict.items()}\n",
    "    return trace_Z, trace_E, trace_N\n",
    "\n",
    "\n",
    "waveform_types = [waveform_type[i] for i in range(7)]\n",
    "dataframes = [df_mag12, df_mag3p1, df_mag3p2, df_mag4p1, df_mag4p2, df_mag5, df_mag69]\n",
    "\n",
    "traces = [create_traces(event_dict, df, waveform, list_closest_stations) for waveform, df in zip(waveform_types, dataframes)]\n",
    "\n",
    "trace_mag12_v2_Z, trace_mag12_v2_E, trace_mag12_v2_N = traces[0]\n",
    "trace_mag3p1_v2_Z, trace_mag3p1_v2_E, trace_mag3p1_v2_N = traces[1]\n",
    "trace_mag3p2_v2_Z, trace_mag3p2_v2_E, trace_mag3p2_v2_N = traces[2]\n",
    "trace_mag4p1_v2_Z, trace_mag4p1_v2_E, trace_mag4p1_v2_N = traces[3]\n",
    "trace_mag4p2_v2_Z, trace_mag4p2_v2_E, trace_mag4p2_v2_N = traces[4]\n",
    "trace_mag5_v2_Z, trace_mag5_v2_E, trace_mag5_v2_N = traces[5]\n",
    "trace_mag69_v2_Z, trace_mag69_v2_E, trace_mag69_v2_N = traces[6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "source": [
    "trace_mag69_v2_Z[5159059]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "# put trace_mag4p1 and trace_mag4p2 together in one list called trace_mag4\n",
    "trace_mag3_v2_Z = {**trace_mag3p1_v2_Z, **trace_mag3p2_v2_Z}\n",
    "trace_mag3_v2_E = {**trace_mag3p1_v2_E, **trace_mag3p2_v2_E}\n",
    "trace_mag3_v2_N = {**trace_mag3p1_v2_N, **trace_mag3p2_v2_N}\n",
    "\n",
    "trace_mag4_v2_Z = {**trace_mag4p1_v2_Z, **trace_mag4p2_v2_Z}\n",
    "trace_mag4_v2_E = {**trace_mag4p1_v2_E, **trace_mag4p2_v2_E}\n",
    "trace_mag4_v2_N = {**trace_mag4p1_v2_N, **trace_mag4p2_v2_N}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "\n",
    "def calculate_power(traces):\n",
    "    _, power_events = zip(*[energy_power(st.data) for st in traces.values()])\n",
    "    return power_events\n",
    "\n",
    "traces_ch_Z = [trace_mag12_v2_Z, trace_mag3_v2_Z, trace_mag4_v2_Z, trace_mag5_v2_Z, trace_mag69_v2_Z]\n",
    "traces_ch_E = [trace_mag12_v2_E, trace_mag3_v2_E, trace_mag4_v2_E, trace_mag5_v2_E, trace_mag69_v2_E]\n",
    "traces_ch_N = [trace_mag12_v2_N, trace_mag3_v2_N, trace_mag4_v2_N, trace_mag5_v2_N, trace_mag69_v2_N]\n",
    "power_events_Z = [calculate_power(trace) for trace in traces_ch_Z]\n",
    "power_events_E = [calculate_power(trace) for trace in traces_ch_E]\n",
    "power_events_N = [calculate_power(trace) for trace in traces_ch_N]\n",
    "\n",
    "power_events_mag12_Z, power_events_mag3_Z, power_events_mag4_Z, power_events_mag5_Z, power_events_mag69_Z = power_events_Z\n",
    "power_events_mag12_E, power_events_mag3_E, power_events_mag4_E, power_events_mag5_E, power_events_mag69_E = power_events_E\n",
    "power_events_mag12_N, power_events_mag3_N, power_events_mag4_N, power_events_mag5_N, power_events_mag69_N = power_events_N\n",
    "\n",
    "power_events_mag12 = [power_events_mag12_Z, power_events_mag12_E, power_events_mag12_N]\n",
    "power_events_mag3 = [power_events_mag3_Z, power_events_mag3_E, power_events_mag3_N]\n",
    "power_events_mag4 = [power_events_mag4_Z, power_events_mag4_E, power_events_mag4_N]\n",
    "power_events_mag5 = [power_events_mag5_Z, power_events_mag5_E, power_events_mag5_N]\n",
    "power_events_mag69 = [power_events_mag69_Z, power_events_mag69_E, power_events_mag69_N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "print('Cantidad de eventos para cada magnitud:')\n",
    "print(f'M<3: {len(power_events_mag12_Z)}')\n",
    "print(f'3<=M<4: {len(power_events_mag3_Z)}')\n",
    "print(f'4<=M<5: {len(power_events_mag4_Z)}')\n",
    "print(f'5<=M<6: {len(power_events_mag5_Z)}')\n",
    "print(f'M>=6: {len(power_events_mag69_Z)}')\n",
    "print('Total de eventos:', len(power_events_mag12_Z) + len(power_events_mag3_Z) + len(power_events_mag4_Z) + len(power_events_mag5_Z) + len(power_events_mag69_Z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Histogramas para cada canal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Canal Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "power_all_mags_Z = [power_events_mag12_Z, power_events_mag3_Z, power_events_mag4_Z, power_events_mag5_Z, power_events_mag69_Z]\n",
    "\n",
    "power_over_3_Z = list(itertools.chain(power_events_mag3_Z, power_events_mag4_Z, power_events_mag5_Z, power_events_mag69_Z))\n",
    "\n",
    "power_over_4_Z = list(itertools.chain(power_events_mag4_Z, power_events_mag5_Z, power_events_mag69_Z))\n",
    "power_under_4_Z = list(itertools.chain(power_events_mag12_Z, power_events_mag3_Z))\n",
    "\n",
    "power_over_5_Z = list(itertools.chain(power_events_mag5_Z, power_events_mag69_Z))\n",
    "power_under_5_Z = list(itertools.chain(power_under_4_Z, power_events_mag4_Z))\n",
    "\n",
    "power_under_6_Z = list(itertools.chain(power_under_5_Z, power_events_mag5_Z))\n",
    "\n",
    "power_sep_3_Z = [power_events_mag12_Z, power_over_3_Z]\n",
    "power_sep_4_Z = [power_under_4_Z, power_over_4_Z]\n",
    "power_sep_5_Z = [power_under_5_Z, power_over_5_Z]\n",
    "power_sep_6_Z = [power_under_6_Z, power_events_mag69_Z]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<3','M>=3']\n",
    "plot_power(power_sep_3_Z, station = '', channel = 'Z', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim= [4,17],y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<4','M>=4']\n",
    "plot_power(power_sep_4_Z, station = '', channel = 'Z', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[4,17] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<5','M>=5']\n",
    "plot_power(power_sep_5_Z, station = '', channel = 'Z', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[4,17],y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<6','M>=6']\n",
    "plot_power(power_sep_6_Z, station = '', channel = 'Z', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[4,17] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<3','M>=6']\n",
    "plot_power([power_events_mag12_Z, power_events_mag69_Z], station = '', channel = 'Z', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[4,17] ,y_lim=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<4','M>=6']\n",
    "plot_power([power_under_4_Z, power_events_mag69_Z], station = '', channel = 'Z', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[4,17] ,y_lim=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<3','M>=3','M>=4', 'M>=5', 'M>=6']\n",
    "plot_power(power_all_mags_Z, station = '', channel = 'Z', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[4,17] ,y_lim=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Canal E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "power_all_mags_E = [power_events_mag12_E, power_events_mag3_E, power_events_mag4_E, power_events_mag5_E, power_events_mag69_E]\n",
    "\n",
    "power_over_3_E = list(itertools.chain(power_events_mag3_E, power_events_mag4_E, power_events_mag5_E, power_events_mag69_E))\n",
    "\n",
    "power_over_4_E = list(itertools.chain(power_events_mag4_E, power_events_mag5_E, power_events_mag69_E))\n",
    "power_under_4_E = list(itertools.chain(power_events_mag12_E, power_events_mag3_E))\n",
    "\n",
    "power_over_5_E = list(itertools.chain(power_events_mag5_E, power_events_mag69_E))\n",
    "power_under_5_E = list(itertools.chain(power_under_4_E, power_events_mag4_E))\n",
    "\n",
    "power_under_6_E = list(itertools.chain(power_under_5_E, power_events_mag5_E))\n",
    "\n",
    "power_sep_3_E = [power_events_mag12_E, power_over_3_E]\n",
    "power_sep_4_E = [power_under_4_E, power_over_4_E]\n",
    "power_sep_5_E = [power_under_5_E, power_over_5_E]\n",
    "power_sep_6_E = [power_under_6_E, power_events_mag69_E]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<3','M>=3']\n",
    "plot_power(power_sep_3_E, station = '', channel = 'E' ,n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim= [20,35] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<4','M>=4']\n",
    "plot_power(power_sep_4_E, station = '', channel = 'E', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim= [20,35] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<5','M>=5']\n",
    "plot_power(power_sep_5_E, station = '', channel = 'E', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim= [20,35] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<6','M>=6']\n",
    "plot_power(power_sep_6_E, station = '', channel = 'E', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[20,35] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<3','M>=5']\n",
    "plot_power([power_events_mag12_E, power_events_mag5_E], station = '', channel = 'E', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[20,35] ,y_lim=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<4','M>=5']\n",
    "plot_power([power_under_4_E, power_events_mag5_E], station = '', channel = 'E' ,n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[20,35] ,y_lim=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<3','M>=3','M>=4', 'M>=5', 'M>=6']\n",
    "plot_power(power_all_mags_E, station = '', channel = 'E' ,n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[20,35] ,y_lim=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Canal N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "power_all_mags_N = [power_events_mag12_N, power_events_mag3_N, power_events_mag4_N, power_events_mag5_N, power_events_mag69_N]\n",
    "\n",
    "power_over_3_N = list(itertools.chain(power_events_mag3_N, power_events_mag4_N, power_events_mag5_N, power_events_mag69_N))\n",
    "\n",
    "power_over_4_N = list(itertools.chain(power_events_mag4_N, power_events_mag5_N, power_events_mag69_N))\n",
    "power_under_4_N = list(itertools.chain(power_events_mag12_N, power_events_mag3_N))\n",
    "\n",
    "power_over_5_N = list(itertools.chain(power_events_mag5_N, power_events_mag69_N))\n",
    "power_under_5_N = list(itertools.chain(power_under_4_N, power_events_mag4_N))\n",
    "\n",
    "power_under_6_N = list(itertools.chain(power_under_5_N, power_events_mag5_N))\n",
    "\n",
    "power_sep_3_N = [power_events_mag12_N, power_over_3_N]\n",
    "power_sep_4_N = [power_under_4_N, power_over_4_N]\n",
    "power_sep_5_N = [power_under_5_N, power_over_5_N]\n",
    "power_sep_6_N = [power_under_6_N, power_events_mag69_N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<3','M>=3']\n",
    "plot_power(power_sep_3_N, station = '', channel = 'N' ,n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim= [20,35] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<4','M>=4']\n",
    "plot_power(power_sep_4_N, station = '', channel = 'N', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim= [20,35] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<5','M>=5']\n",
    "plot_power(power_sep_5_N, station = '', channel = 'N', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim= [20,35] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<6','M>=6']\n",
    "plot_power(power_sep_6_N, station = '', channel = 'N', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[20,35] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<3','M>=5']\n",
    "plot_power([power_events_mag12_N, power_events_mag5_N], station = '', channel = 'N', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[20,35] ,y_lim=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<4','M>=5']\n",
    "plot_power([power_under_4_N, power_events_mag5_N], station = '', channel = 'N', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[20,35] ,y_lim=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "event_type = ['M<3','M>=3','M>=4', 'M>=5', 'M>=6']\n",
    "plot_power(power_all_mags_N, station = '', channel = 'N' ,n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[20,35] ,y_lim=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Ver tiempo de inicio eventos mayores a 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "# calculate sta/lta algo for the whole trace_mag69_v2_Z list\n",
    "trace_mag69_v2_Z = {k: v.trim(starttime=v.stats.starttime + 1, pad=True, fill_value=0) for k, v in trace_mag69_v2_Z.items()} # este código saca el primer segundo, esto ya que en el primer segundo hay como un pick extraño\n",
    "trace_mag69_v2_Z_stream = [Stream(traces = trace) for trace in trace_mag69_v2_Z.values()]\n",
    "sta_lta_mag6 = [p_picking_each(trace, ventana_10s = 10, ventana_30s = 30, nsta = 3, nlta = 13, thr_on = 4.2, thr_off = 1) for trace in trace_mag69_v2_Z_stream]\n",
    "\n",
    "ic(len(trace_mag69_v2_Z_stream))\n",
    "ic(len(sta_lta_mag6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hasta ahora problemáticos M>6:\n",
    "- 4597528: Sin sta/lta, magnitud 7, mis ideas son que o la señal tiene mucho ruido o fue descargada la traza cuando ya había empezado el evento\n",
    "- 5159022: Tres trigger, magnitud 8.3, llegada de diferentes ondas?\n",
    "- 5159026: No hay trigger, pareceria haber mucho ruido\n",
    "- 5159547: Se producen dos trigger, tendría sentido tomar el primero como onda P y el segundo como onda S??\n",
    "- 10100470: Lo mismo de antes.\n",
    "- 10997608: IDEM\n",
    "- 11303695: IDEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "source": [
    "def analize_sta_lta(sta_lta: list, trace: dict, duration: float, off: bool = False):\n",
    "    tr = list(trace.values())[0]\n",
    "    key = list(trace.keys())[0]\n",
    "    if sta_lta is not None:\n",
    "        fig, ax = plt.subplots(figsize=(7, 3))\n",
    "        ax.plot(tr.times(), tr.data, 'k')\n",
    "        ax.set_title(f'Canal: {tr.stats.channel}. EventId: {key}. Station: {tr.stats.station}')\n",
    "        ax.set_xlabel('Time [s]')\n",
    "        ax.set_ylabel('Amplitude')\n",
    "        #ax.set_xlim(0, 100)\n",
    "        # add a vertical line where the cft array surpasses the threshold of 4.3\n",
    "        ax.axvline(sta_lta - tr.stats.starttime ,  color='r', linestyle='--', label='ON')\n",
    "        if off:\n",
    "            ax.axvline(sta_lta - tr.stats.starttime + duration ,  color='b', linestyle='--', label='OFF')\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "source": [
    "# Obtener las ID y data de los eventos\n",
    "output_folder = os.path.join('BD paper', 'catalogos', 'plots_señales_6')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "keys = list(trace_mag69_v2_Z.keys())\n",
    "values = list(trace_mag69_v2_Z.values())\n",
    "plt.rcParams['figure.constrained_layout.use'] = True\n",
    "\n",
    "sta_lta_mag6_clean = {}\n",
    "count= 0\n",
    "for i in range(len(sta_lta_mag6)):\n",
    "    key = keys[i]\n",
    "    value = values[i]\n",
    "    if sta_lta_mag6[i]:\n",
    "        count += 1\n",
    "        sta_lta_mag6_clean[key] = sta_lta_mag6[i][0]\n",
    "        # Extraer los datos y el tiempo del objeto de ObsPy\n",
    "        data = value.data\n",
    "        time = value.times('matplotlib')\n",
    "        \n",
    "        # Crear una figura y un eje con matplotlib\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))  # Cambiar el tamaño de la figura\n",
    "        \n",
    "        # Trazar los datos\n",
    "        ax.plot_date(time, data, '-')\n",
    "        \n",
    "        # Ajustar los ejes\n",
    "        ax.autoscale_view()\n",
    "        fig.autofmt_xdate()\n",
    "        #Agregar titulo\n",
    "        ax.set_title(f'Canal: {value.stats.channel}. EventId: {key} ')\n",
    "\n",
    "        # Agregar etiquetas a los ejes\n",
    "        ax.set_xlabel('Tiempo')\n",
    "        ax.set_ylabel('Amplitud')\n",
    "        \n",
    "        # Configurar las marcas del eje x para que aparezcan cada 30 segundos\n",
    "        ax.xaxis.set_major_locator(mdates.SecondLocator(interval=30))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "        \n",
    "        # Guardar la figura\n",
    "        plot_path = os.path.join(output_folder, f'evento_{key}.png')\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "source": [
    "tr_mag69_list[idx].stats.starttime - 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "source": [
    "idx = 16\n",
    "tr_mag69_list = list(trace_mag69_v2_Z.values())\n",
    "ic(sta_lta_mag6[idx])\n",
    "ic(tr_mag69_list[idx])\n",
    "tr_mag69_list[idx].trim(starttime = tr_mag69_list[idx].stats.starttime-60).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "source": [
    "co03 = (-30.839,-70.689)\n",
    "event = (-31.5622, -71.4262)\n",
    "\n",
    "dist = geodesic(co03, event).km\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tiempos de incio quedaron guardados en el diccionario sta_lta_mag6_clean y en la lista . Esto en los casos donde las señales que son triggereadas por sta/lta. Hay 2 eventos que no fueron triggereados,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Cálculo del fin del evento.\n",
    "Esto se hace considerando cuando la energía baja del 3% de su peak por frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "source": [
    "ic(len(sta_lta_mag6_clean))\n",
    "ic(len(sta_lta_mag6))\n",
    "ic(len(trace_mag69_v2_Z))\n",
    "ic(len(trace_mag69_v2_Z_stream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "# Se calcula el punto donde cada traza tendría su finalización del evento\n",
    "end_event = [endpoint_event(value.data, thr_energy = 0.97)[1] for value in values]\n",
    "peak_enegy = [endpoint_event(value.data)[0] for value in values]\n",
    "\n",
    "sliced_trs = []\n",
    "duration = []\n",
    "j = 0\n",
    "k = []\n",
    "for i in range(len(trace_mag69_v2_Z_stream)):\n",
    "    if sta_lta_mag6[i]:\n",
    "        sliced_trs.append(trace_mag69_v2_Z_stream[i].slice(starttime = sta_lta_mag6[i][0], endtime=sta_lta_mag6[i][0] + end_event[i]/40))\n",
    "        duration.append(sliced_trs[j][0].stats.endtime - sliced_trs[j][0].stats.starttime)\n",
    "        j += 1\n",
    "    else:\n",
    "        k.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "source": [
    "# Se grafican las trazas cortadas\n",
    "sliced_trs[5].plot()\n",
    "print('Start time:', sliced_trs[12][0].stats.starttime)\n",
    "print('End time:', sliced_trs[12][0].stats.endtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "source": [
    "# Obtener las ID y data de los eventos\n",
    "keys = list(trace_mag69_v2_Z.keys())\n",
    "values = list(trace_mag69_v2_Z.values())\n",
    "\n",
    "sta_lta_mag6_clean = {}\n",
    "count= 0\n",
    "j = 0\n",
    "for i in range(len(sta_lta_mag6)):\n",
    "    key = keys[i]\n",
    "    value = values[i]\n",
    "    if sta_lta_mag6[i]:\n",
    "        count += 1\n",
    "        sta_lta_mag6_clean[key] = sta_lta_mag6[i][0]\n",
    "        analize_sta_lta(sta_lta_mag6[i][0], {key: value}, duration[j], off=True)\n",
    "        j += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decaimiento del 3% del frame con la energía máxima "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "source": [
    "dur = np.array(duration)\n",
    "ic(np.mean(dur).round(2))\n",
    "# calcula la desviacion estandar de dur\n",
    "ic(np.std(dur).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "source": [
    "path_to_duration = os.path.join('BD paper', 'catalogos', 'duracion_eventos.xlsx')\n",
    "# take the column \"Duracion(s)\" and calculate the mean and std\n",
    "df_to_change = pd.read_excel(path_to_duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "source": [
    "# Calculate the mean and standard deviation\n",
    "mean_dur = np.mean(df_to_change['Duracion(s)']).round(2)\n",
    "std_dur = np.std(df_to_change['Duracion(s)']).round(2)\n",
    "max_dur = np.max(df_to_change['Duracion(s)']).round(2)\n",
    "min_dur = np.min(df_to_change['Duracion(s)']).round(2)\n",
    "\n",
    "# Add a new column 'Promedio' and 'Std' and set the first cell to the calculated values\n",
    "df_to_change.at[0, 'Promedio'] = mean_dur\n",
    "df_to_change.at[0, 'Sismo más largo'] = max_dur\n",
    "df_to_change.at[0, 'Sismo más corto'] = min_dur\n",
    "df_to_change.at[0, 'Std'] = std_dur\n",
    "\n",
    "# Save the modified DataFrame back to an Excel file\n",
    "df_to_change.to_excel(path_to_duration , index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cantidad de eventos para cada magnitud:\n",
    "M<3: 123 (125 en excel)\n",
    "3<=M<4: 113 (125 en excel)\n",
    "4<=M<5: 92 (99 en excel)\n",
    "5<=M<6: 103 (111 en excel)\n",
    "M>=6: 40 (46 en excel)\n",
    "Total de eventos: 471 (506)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Gaussian Mixtures Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESto de abajo es un EJEMPLO GENÉRICO del uso de GMM de la librería sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "source": [
    "classes = ['M<3', 'M>=5']\n",
    "\n",
    "power_all_mags_Z = [power_events_mag12_Z, power_events_mag3_Z, power_events_mag4_Z, power_events_mag5_Z, power_events_mag69_Z]\n",
    "\n",
    "power_to_test = [power_events_mag12_Z, power_events_mag5_Z ,power_events_mag69_Z]\n",
    "\n",
    "power_last_frame = [arr[-1] for tup in power_to_test  for arr in tup]\n",
    "\n",
    "X = np.array(power_last_frame).reshape(-1,1)\n",
    "X = np.log10(X)\n",
    "\n",
    "y= np.concatenate([np.zeros(len(power_all_mags_Z[0])),\n",
    "                         np.ones(len(power_all_mags_Z[3])),\n",
    "                         np.ones(len(power_all_mags_Z[4]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import classification_report\n",
    "import scipy.stats as stats\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "# Supongamos que 'X' son tus datos y 'y' son las clases\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Normalizar los datos\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "# Crear un GMM con 2 componentes para cada clase\n",
    "#gmm = GaussianMixture(n_components=2)\n",
    "\n",
    "# Ajustar el GMM a los datos de entrenamiento\n",
    "g = GaussianMixture(n_components=2,covariance_type='full')\n",
    "\n",
    "g.fit(X)\n",
    "weights = g.weights_\n",
    "means = g.means_\n",
    "covars = g.covariances_\n",
    "\n",
    "\n",
    "# Predecir las clases de los datos de prueba\n",
    "y_pred = g.predict(X_test)\n",
    "\n",
    "# Calcular las métricas de clasificación\n",
    "#report = classification_report(y_test, y_pred)\n",
    "#print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir las clases\n",
    "classes = ['M<3', 'M>=5']\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Crear un mapa de calor de la matriz de confusión usando seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "heatmap = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', linewidths=.5, cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "# Añadir etiquetas personalizadas al eje x e y\n",
    "heatmap.set_xticklabels(classes, rotation=45, ha='right', fontsize=12)\n",
    "heatmap.set_yticklabels(classes, rotation=0, fontsize=12)\n",
    "\n",
    "plt.title('Matriz de confusión')\n",
    "plt.xlabel('Clase predicha')\n",
    "plt.ylabel('Clase verdadera')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "source": [
    "# Crear un histograma de las clases predichas\n",
    "plt.hist(y_pred, bins=2, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.title('Histograma de las clases predichas')\n",
    "plt.xlabel('Clase')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "source": [
    "\n",
    "# Crear un gráfico de densidad de las potencias de los eventos sísmicos\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Densidad de las potencias de los eventos sísmicos')\n",
    "colors = ['blue', 'orange']  # Definir los colores para cada clase\n",
    "for i in range(2):\n",
    "    mask = (y_pred == i)  # Crear una máscara booleana\n",
    "    subset = X_test[mask]  # Aplicar la máscara a X_test\n",
    "    subset = np.squeeze(subset)  # Eliminar las dimensiones de tamaño uno\n",
    "    if len(subset) > 0:\n",
    "        plt.hist(subset, bins=30, density=False, alpha=0.5, color=colors[i], label=f'Clase {i}')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])  # Cambiar la escala del eje y\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Crear un mapa de calor de la matriz de confusión\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matriz de confusión')\n",
    "plt.xlabel('Clase predicha')\n",
    "plt.ylabel('Clase verdadera')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "source": [
    "\n",
    "# Supongamos que 'potencias' es una lista de potencias y 'clases' es una lista de clases\n",
    "potencias = [10.1, 12.9, 10.0, 13.1, 10.2]\n",
    "clases = ['M<4', 'M>=4', 'M<4', 'M>=4', 'M<4']\n",
    "\n",
    "# Convertir las listas a arrays de numpy\n",
    "X = np.array(potencias).reshape(-1, 1)  # reshape(-1, 1) es necesario porque 'X' debe ser una matriz bidimensional\n",
    "y = np.array(clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Crear un GMM con 2 componentes para cada clase\n",
    "gmm_M_menor_4 = GaussianMixture(n_components=2)\n",
    "gmm_M_mayor_igual_4 = GaussianMixture(n_components=2)\n",
    "\n",
    "# Ajustar los GMM a tus datos (supongamos que 'datos_M_menor_4' y 'datos_M_mayor_igual_4' son tus datos)\n",
    "gmm_M_menor_4.fit(datos_M_menor_4)\n",
    "gmm_M_mayor_igual_4.fit(datos_M_mayor_igual_4)\n",
    "\n",
    "def clasificar_gmm(potencia):\n",
    "    # Calcular la probabilidad de cada clase\n",
    "    prob_M_menor_4 = np.exp(gmm_M_menor_4.score_samples([potencia]))\n",
    "    prob_M_mayor_igual_4 = np.exp(gmm_M_mayor_igual_4.score_samples([potencia]))\n",
    "\n",
    "    # Clasificar en la clase que tenga la mayor probabilidad\n",
    "    if prob_M_menor_4 > prob_M_mayor_igual_4:\n",
    "        return 'M<4'\n",
    "    else:\n",
    "        return 'M>=4'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trigger_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
