{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "# %matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lptv/Roberto/Trigger/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from itertools import product\n",
    "from itertools import groupby\n",
    "import itertools\n",
    "import pickle\n",
    "import glob\n",
    "import chardet\n",
    "import copy\n",
    "from natsort import natsorted\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()\n",
    "# Set a professional style for the plot\n",
    "#plt.style.use('seaborn')\n",
    "import matplotlib.dates as mdaates\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.cm as cm\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset, plot_trigger\n",
    "from obspy import Trace, Stream\n",
    "from obspy.imaging.spectrogram import spectrogram\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from tqdm.auto import tqdm\n",
    "from icecream import ic\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm, kstest, skew, kurtosis\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Local Imports\n",
    "from energy.utils_general import *\n",
    "from energy.utils_energy import *\n",
    "from energy.preprocessing import *\n",
    "from energy.metrics import *\n",
    "from picking.p_picking import p_picking_each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tener en consideración. Las rutas a los archivos están guardadas de la siguiente forma: *waveform_M/event_ID/channel/station/eventid_canal_horainicio_horatermino.mseed*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creamos un diccionario con las coordenadas de las estaciones del litoral a partir del archivo csv com las estaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_P = 8.064\n",
    "file_stations_coord = os.path.join(\"BD paper\", \"catalogos\",\"Estaciones_Chile.csv\")\n",
    "df = pd.read_csv(file_stations_coord, sep=';')\n",
    "stations_coord_all = df.set_index('Station')[['Latitude', 'Longitude']].apply(tuple, axis=1).to_dict()\n",
    "stations_names = list(stations_coord_all.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Trabajar con la base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. La función all_channel_dict entrega un nested dictionary donde la primera capa de llaves indica el rango de magnitud, la segunda capa el ID evento, la tercera la red de la estación que la captó, y finalmente el valor es la traza ya leída por obspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to all the folders from the folder BD paper that is in the current directory\n",
    "folders = glob.glob(os.path.join('BD paper', '*'))\n",
    "# Get all the folders in the BD paper folder that start with waveform \n",
    "folders_signals = [folder for folder in folders if folder.startswith(os.path.join('BD paper', 'waveform'))]\n",
    "folders_signals = sorted(folders_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BD paper/waveform_1-2.9',\n",
       " 'BD paper/waveform_3p1',\n",
       " 'BD paper/waveform_3p2',\n",
       " 'BD paper/waveform_4p1',\n",
       " 'BD paper/waveform_4p2',\n",
       " 'BD paper/waveform_5',\n",
       " 'BD paper/waveform_6-9']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders_signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función se puede/debe optimizar. Usando os.walk probablemente sale más rápido. No sé si existe alguna forma de hacerla más generalizable eso si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inventory_path = \"inventory\"\n",
    "#inventory_path = os.path.join('BD paper', 'stations_xml1')\n",
    "inventory_path = os.path.join('inventory2')\n",
    "def all_channels_dict(folders_signals, inventory_path=inventory_path):\n",
    "\n",
    "    result_dict = OrderedDict()\n",
    "\n",
    "    for folder in folders_signals:\n",
    "        waveform_type = os.path.split(folder)[-1]\n",
    "        events_id = natsorted(glob.glob(os.path.join(folder, '*')))\n",
    "\n",
    "        for event_id in events_id:\n",
    "            events_name = os.path.split(event_id)[-1]\n",
    "            network_path = glob.glob(os.path.join(event_id, '*'))\n",
    "\n",
    "            for network_folder in network_path:\n",
    "                network_name = os.path.split(network_folder)[-1]\n",
    "                \n",
    "                stations_path = glob.glob(os.path.join(network_folder, '*'))\n",
    "\n",
    "                for station_path in stations_path:\n",
    "                    stations_names = os.path.split(station_path)[-1]\n",
    "                    #ic(stations_names)\n",
    "                    stations_path = glob.glob(os.path.join(station_path, '*'))\n",
    "                    #ic(stations_path)\n",
    "\n",
    "                    # Get the channels for each station\n",
    "                    stations_ch_z = [station_z for station_z in stations_path if 'BHZ' in station_z]\n",
    "                    stations_ch_e = [station_e for station_e in stations_path if 'BHE' in station_e]\n",
    "                    stations_ch_n = [station_n for station_n in stations_path if 'BHN' in station_n]\n",
    "\n",
    "                    # if len(stations_ch_e) == 0 or len(stations_ch_n) == 0:\n",
    "                    #     break\n",
    "                    \n",
    "                    traces = []\n",
    "\n",
    "                    # Files to remove response\n",
    "                    file_response = glob.glob(os.path.join(inventory_path, f\"{stations_names}.xml\"))\n",
    "                    \n",
    "                    if file_response:\n",
    "                        remove_file = file_response[0]\n",
    "                    \n",
    "\n",
    "                    trace = read(stations_ch_z[0])\n",
    "\n",
    "                    if len(stations_ch_e) > 0:\n",
    "                        trace += read(stations_ch_e[0])\n",
    "                    if len(stations_ch_n) > 0:\n",
    "                        trace += read(stations_ch_n[0])\n",
    "\n",
    "                    tr_resp = trace.copy()\n",
    "                    #ic(len(tr_resp))\n",
    "                    for i in range(len(tr_resp)):\n",
    "                        try:\n",
    "                            st_removed = remove_response(tr_resp.select(channel=tr_resp[i].stats.channel)[0], remove_file , 'obspy')\n",
    "                            tr_resp[i] = st_removed\n",
    "                        except:\n",
    "                            ic(station_path)\n",
    "                            #ic(tr_resp[i].stats.station)\n",
    "                            pass\n",
    "\n",
    "                    tr_filtered = tr_resp.copy()\n",
    "                    tr_filtered.filter('bandpass', freqmin=4.0, freqmax=10.0)\n",
    "                    traces.append(tr_filtered)\n",
    "\n",
    "                    result_dict.setdefault(waveform_type, {}).setdefault(events_name, {}).setdefault(network_name, {}).setdefault(stations_names, traces)\n",
    "\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "event_dict = all_channels_dict(folders_signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CX: PB11\n",
    "- C: ROC1\n",
    "- C1: TA02, AC05, VA05\n",
    "\n",
    "Lo que hice por ahora fue subirle 10 años al endDate para que no me tire error. A ROC1 le baje 10 años al startDate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_path = 'BD paper/waveform_4p1/5159607/C/ROC1'\n",
    "stations_path = glob.glob(os.path.join(tr_path, '*'))\n",
    "remove_file = 'BD paper/stations_xml1/PB11.xml'\n",
    "\n",
    "trace = read(stations_path[0])\n",
    "trace += read(stations_path[1])\n",
    "trace += read(stations_path[2])\n",
    "tr_resp = trace.copy()\n",
    "tr_resp[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tr_resp)):\n",
    "\n",
    "    # if tr_resp[i].stats.station in ['PB11', 'TA02','AC05', 'ROC1', 'VA05']:\n",
    "    #     ic(remove_file)\n",
    "    #     ic(station_path)\n",
    "    #     break\n",
    "    #ic(tr_resp[i].stats.station)\n",
    "    try:\n",
    "        st_removed = remove_response(tr_resp.select(channel=tr_resp[i].stats.channel)[0], remove_file , 'obspy')\n",
    "        tr_resp[i] = st_removed\n",
    "    except:\n",
    "        ic(tr_resp[i].stats.station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_type = list(event_dict.keys())\n",
    "\n",
    "# get all the events ids consideting waveform type is a list. And make events_ids a dictionary with the waveform type as keys\n",
    "events_ids = {waveform_type: [int(event_id) for event_id in event_dict[waveform_type].keys()] for waveform_type in waveform_type}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. La siguiente función pone a todas las traces de TODOS los eventos en una lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_traces(nested_dict):\n",
    "    traces = []\n",
    "    for value in nested_dict.values():\n",
    "        if isinstance(value, dict):\n",
    "            traces.extend(get_all_traces(value))\n",
    "        else:\n",
    "            traces.extend(value)\n",
    "    return traces\n",
    "\n",
    "all_traces = get_all_traces(event_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 El siguiente código es para crear un nuevo archivo CSV que contenga solo los siguientes datos:\n",
    "- EventId\n",
    "- Time\n",
    "- Latitud\n",
    "- Longitud\n",
    "- Magnitud\n",
    "- Estación (la que aparece automáticamente en el catálogo)\n",
    "- Estaciones más cercanas (calculada con geodisic. Cantidad arbitraria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the csv files in the BD paper folder that have the word \"cercanos\" in their name\n",
    "files_raw = sorted(glob.glob(os.path.join('BD paper', 'catalogos','*descargados*.csv')))\n",
    "files_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_csv(files_raw, events_ids):\n",
    "    # Columnas a mantener\n",
    "    columns_to_keep = [\"#EventID\", \"Time\", \"Latitude\", \"Longitude\", \"Magnitude\", \"Estacion\"]\n",
    "\n",
    "    for i, (waveform_type, event_ids) in enumerate(events_ids.items()):\n",
    "        # Leer el archivo CSV\n",
    "        df = pd.read_csv(files_raw[i], sep=';')\n",
    "\n",
    "        # Filtrar las filas donde '#EventID' está en event_ids\n",
    "        df = df[df['#EventID'].isin(event_ids)]\n",
    "\n",
    "        # Ordernar la columna de #EventID\n",
    "        df = df.sort_values(by='#EventID')\n",
    "\n",
    "        # Mantener solo las columnas deseadas\n",
    "        df = df[columns_to_keep]\n",
    "\n",
    "        # Guardar el nuevo DataFrame en un nuevo archivo CSV\n",
    "        df.to_excel(os.path.join(\"BD paper\", \"catalogos\", f\"{waveform_type}_filtered.xlsx\"), index=False)\n",
    "\n",
    "filter_csv(files_raw, events_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Ahora que ya se tienen archivos .csv con columnas \"Event\"Time\", \"Magnitud\", \"Estación\", \"Latitud\", \"Longitud\", podemos verificar efectivamente cuál es la estación más cercana y agregarla como última columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_filtered = sorted(glob.glob(os.path.join('BD paper', 'catalogos','*filtered*')))\n",
    "files_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función además agrega columnas con el tiempo estimado donde el sismo debería verse en la i-ésima estación más cercana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_station(file_path, stations_coord_all, v_P, n_closest_stations = 7):\n",
    "    '''\n",
    "    Actualiza todos los archivos csv con la estación más cercana a cada evento sísmico.\n",
    "    '''\n",
    "\n",
    "    for file in file_path:\n",
    "        ic(file)\n",
    "\n",
    "        df_events = pd.read_excel(file)\n",
    "\n",
    "        # Cambiar nombre de la columna \"Time\" a \"Fecha UTC\" y cosas en inglés por español\n",
    "        df_events = df_events.rename(columns={'Time': 'Fecha UTC', 'Latitude': 'Latitud', 'Longitude': 'Longitud', 'Magnitude': 'Magnitud'})\n",
    "        \n",
    "        # Cambiar esta columna a formato UTC\n",
    "        df_events['Fecha UTC'] = pd.to_datetime(df_events['Fecha UTC'])\n",
    "        #df_events = df_events.rename(columns={'Latitude': 'Latitud', 'Longitude': 'Longitud'})\n",
    "\n",
    "        df_events.to_excel(file, index=False)\n",
    "\n",
    "        # Tomar el valor máximo y mínimo de la magnitud en el dataframe\n",
    "        max_magnitude = df_events['Magnitud'].max()\n",
    "        min_magnitude = df_events['Magnitud'].min()\n",
    "\n",
    "        df_new = calculate_detection_times(df_events, stations_coord_all, v_P, magnitude_range=(min_magnitude, max_magnitude))\n",
    "        _, closest_sts_names = nearest_n_stations(df_new, stations_names, n_closest_stations)\n",
    "\n",
    "        df_events = pd.read_excel(file)\n",
    "\n",
    "        # Agregar columnas para cada estación cercana\n",
    "        for i in range(n_closest_stations):\n",
    "            col_name = f'Estación más cercana {i+1}'\n",
    "            df_events[col_name] = closest_sts_names[i]\n",
    "\n",
    "        df_events.to_excel(file, index=False)\n",
    "\n",
    "        df_events = pd.read_excel(file)\n",
    "\n",
    "        # Agregar columnas para el tiempo de inicio de cada estación cercana\n",
    "        for i in range(n_closest_stations):\n",
    "            col_name = f'Inicio estación más cercana {i+1}'\n",
    "            df_events[col_name] = df_events.apply(lambda row: df_new.loc[row.name, f'Inicio_{row[f\"Estación más cercana {i+1}\"]}'], axis=1)\n",
    "\n",
    "        # Escribir en el archivo excel\n",
    "        df_events.to_excel(file, index=False)\n",
    "\n",
    "\n",
    "closest_station(files_filtered, stations_coord_all, v_P)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Procesamiento de las trazas\n",
    "Ahora que ya tenemos los archivos excel con las estaciones más cercanas y con sus estimados tiempos de detección del evento, podemos empezar a trabajar con las trazas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Calculo de la potencia\n",
    "Se deben juntar todas las trazas que pertenecen a cada intervalo de magnitud en una sola lista para luego calcular la potencia. Es decir, todas las que son entre 4 y 5, todas las que son entre 5 y 6, y todas las que son mayores que 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_closest_stations = ['Estación más cercana 1', 'Estación más cercana 2', 'Estación más cercana 3', 'Estación más cercana 4', 'Estación más cercana 5', 'Estación más cercana 6', 'Estación más cercana 7']\n",
    "\n",
    "def find_matching_trace_v2(event_dict, dataframe, waveform_key, station_columns=list_closest_stations):\n",
    "    trace_dict = {} \n",
    "\n",
    "    event_dict_waveform = event_dict[waveform_key]\n",
    "\n",
    "    for i in range(len(dataframe['#EventID'].astype(str))):\n",
    "        station_data = {key: value for sublist in\n",
    "                        list(event_dict_waveform[dataframe['#EventID'].astype(str).iloc[i]].values()) for key, value\n",
    "                        in sublist.items()}\n",
    "        station_names = list(station_data.keys())\n",
    "\n",
    "        for j, column in enumerate(dataframe.loc[:, station_columns]):\n",
    "            station_column_value = dataframe[column].astype(str).iloc[i]\n",
    "            matching_station = next((station for station in station_names if station in station_column_value), None)\n",
    "\n",
    "            if matching_station is not None:\n",
    "                start_time_column = f\"Inicio estación más cercana {j + 1}\"\n",
    "                start_time = UTCDateTime(dataframe[start_time_column].iloc[i])\n",
    "\n",
    "                trace = station_data[matching_station][0]\n",
    "                trace[0].stats.starttime = start_time\n",
    "\n",
    "                # Cambiado trace_list por trace_dict\n",
    "                if dataframe['#EventID'].iloc[i] not in trace_dict:\n",
    "                    trace_dict[dataframe['#EventID'].iloc[i]] = []\n",
    "                trace_dict[dataframe['#EventID'].iloc[i]].extend(station_data[matching_station])\n",
    "\n",
    "                break\n",
    "\n",
    "    return trace_dict\n",
    "\n",
    "df_mag12 = pd.read_excel(files_filtered[0])\n",
    "df_mag3p1 = pd.read_excel(files_filtered[1])\n",
    "df_mag3p2 = pd.read_excel(files_filtered[2])\n",
    "df_mag4p1 = pd.read_excel(files_filtered[3])\n",
    "df_mag4p2 = pd.read_excel(files_filtered[4])\n",
    "df_mag5 = pd.read_excel(files_filtered[5])\n",
    "df_mag69 = pd.read_excel(files_filtered[6])\n",
    "\n",
    "def create_traces(event_dict, dataframe, waveform_key, station_columns):\n",
    "    trace_dict = find_matching_trace_v2(event_dict, dataframe, waveform_key, station_columns)\n",
    "    trace_Z = {event_id: trace[0].select(channel='BHZ')[0] for event_id, trace in trace_dict.items()}\n",
    "    trace_E = {event_id: trace[0].select(channel='BHE')[0] for event_id, trace in trace_dict.items()}\n",
    "    trace_N = {event_id: trace[0].select(channel='BHN')[0] for event_id, trace in trace_dict.items()}\n",
    "    return trace_Z, trace_E, trace_N\n",
    "\n",
    "\n",
    "waveform_types = [waveform_type[i] for i in range(7)]\n",
    "dataframes = [df_mag12, df_mag3p1, df_mag3p2, df_mag4p1, df_mag4p2, df_mag5, df_mag69]\n",
    "\n",
    "traces = [create_traces(event_dict, df, waveform, list_closest_stations) for waveform, df in zip(waveform_types, dataframes)]\n",
    "\n",
    "trace_mag12_v2_Z, trace_mag12_v2_E, trace_mag12_v2_N = traces[0]\n",
    "trace_mag3p1_v2_Z, trace_mag3p1_v2_E, trace_mag3p1_v2_N = traces[1]\n",
    "trace_mag3p2_v2_Z, trace_mag3p2_v2_E, trace_mag3p2_v2_N = traces[2]\n",
    "trace_mag4p1_v2_Z, trace_mag4p1_v2_E, trace_mag4p1_v2_N = traces[3]\n",
    "trace_mag4p2_v2_Z, trace_mag4p2_v2_E, trace_mag4p2_v2_N = traces[4]\n",
    "trace_mag5_v2_Z, trace_mag5_v2_E, trace_mag5_v2_N = traces[5]\n",
    "trace_mag69_v2_Z, trace_mag69_v2_E, trace_mag69_v2_N = traces[6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se juntan las magnitudes de los rangos 3-4 y 4-5 para cada canal.\n",
    "trace_mag3_v2_Z = {**trace_mag3p1_v2_Z, **trace_mag3p2_v2_Z}\n",
    "trace_mag3_v2_E = {**trace_mag3p1_v2_E, **trace_mag3p2_v2_E}\n",
    "trace_mag3_v2_N = {**trace_mag3p1_v2_N, **trace_mag3p2_v2_N}\n",
    "\n",
    "trace_mag4_v2_Z = {**trace_mag4p1_v2_Z, **trace_mag4p2_v2_Z}\n",
    "trace_mag4_v2_E = {**trace_mag4p1_v2_E, **trace_mag4p2_v2_E}\n",
    "trace_mag4_v2_N = {**trace_mag4p1_v2_N, **trace_mag4p2_v2_N}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_power(traces):\n",
    "    _, power_events = zip(*[energy_power(st.data) for st in traces.values()])\n",
    "    return power_events\n",
    "\n",
    "traces_ch_Z = [trace_mag12_v2_Z, trace_mag3_v2_Z, trace_mag4_v2_Z, trace_mag5_v2_Z, trace_mag69_v2_Z]\n",
    "traces_ch_E = [trace_mag12_v2_E, trace_mag3_v2_E, trace_mag4_v2_E, trace_mag5_v2_E, trace_mag69_v2_E]\n",
    "traces_ch_N = [trace_mag12_v2_N, trace_mag3_v2_N, trace_mag4_v2_N, trace_mag5_v2_N, trace_mag69_v2_N]\n",
    "power_events_Z = [calculate_power(trace) for trace in traces_ch_Z]\n",
    "power_events_E = [calculate_power(trace) for trace in traces_ch_E]\n",
    "power_events_N = [calculate_power(trace) for trace in traces_ch_N]\n",
    "\n",
    "power_events_mag12_Z, power_events_mag3_Z, power_events_mag4_Z, power_events_mag5_Z, power_events_mag69_Z = power_events_Z\n",
    "power_events_mag12_E, power_events_mag3_E, power_events_mag4_E, power_events_mag5_E, power_events_mag69_E = power_events_E\n",
    "power_events_mag12_N, power_events_mag3_N, power_events_mag4_N, power_events_mag5_N, power_events_mag69_N = power_events_N\n",
    "\n",
    "power_events_mag12 = [power_events_mag12_Z, power_events_mag12_E, power_events_mag12_N]\n",
    "power_events_mag3 = [power_events_mag3_Z, power_events_mag3_E, power_events_mag3_N]\n",
    "power_events_mag4 = [power_events_mag4_Z, power_events_mag4_E, power_events_mag4_N]\n",
    "power_events_mag5 = [power_events_mag5_Z, power_events_mag5_E, power_events_mag5_N]\n",
    "power_events_mag69 = [power_events_mag69_Z, power_events_mag69_E, power_events_mag69_N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cantidad de eventos para cada magnitud:')\n",
    "print(f'M<3: {len(power_events_mag12_Z)}')\n",
    "print(f'3<=M<4: {len(power_events_mag3_Z)}')\n",
    "print(f'4<=M<5: {len(power_events_mag4_Z)}')\n",
    "print(f'5<=M<6: {len(power_events_mag5_Z)}')\n",
    "print(f'M>=6: {len(power_events_mag69_Z)}')\n",
    "print('Total de eventos:', len(power_events_mag12_Z) + len(power_events_mag3_Z) + len(power_events_mag4_Z) + len(power_events_mag5_Z) + len(power_events_mag69_Z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Histogramas para cada canal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Canal Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_all_mags_Z = [power_events_mag12_Z, power_events_mag3_Z, power_events_mag4_Z, power_events_mag5_Z, power_events_mag69_Z]\n",
    "\n",
    "power_over_3_Z = list(itertools.chain(power_events_mag3_Z, power_events_mag4_Z, power_events_mag5_Z, power_events_mag69_Z))\n",
    "\n",
    "power_over_4_Z = list(itertools.chain(power_events_mag4_Z, power_events_mag5_Z, power_events_mag69_Z))\n",
    "power_under_4_Z = list(itertools.chain(power_events_mag12_Z, power_events_mag3_Z))\n",
    "\n",
    "power_over_5_Z = list(itertools.chain(power_events_mag5_Z, power_events_mag69_Z))\n",
    "power_under_5_Z = list(itertools.chain(power_under_4_Z, power_events_mag4_Z))\n",
    "\n",
    "power_under_6_Z = list(itertools.chain(power_under_5_Z, power_events_mag5_Z))\n",
    "\n",
    "power_sep_3_Z = [power_events_mag12_Z, power_over_3_Z]\n",
    "power_sep_4_Z = [power_under_4_Z, power_over_4_Z]\n",
    "power_sep_5_Z = [power_under_5_Z, power_over_5_Z]\n",
    "power_sep_6_Z = [power_under_6_Z, power_events_mag69_Z]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<3','M>=3']\n",
    "plot_power(power_sep_3_Z, station = '', channel = 'Z' ,n_frames=10, use_log=True,\n",
    "            height = 10, width = 7, event_type=event_type, x_lim= [20,40],y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<4','M>=4']\n",
    "plot_power(power_sep_4_Z, station = '', channel = 'Z', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[4,17] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<5','M>=5']\n",
    "plot_power(power_sep_5_Z, station = '', channel = 'Z', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[4,17],y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<6','M>=6']\n",
    "plot_power(power_sep_6_Z, station = '', channel = 'Z', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[4,17] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<3','M>=6']\n",
    "plot_power([power_events_mag12_Z, power_events_mag69_Z], station = '', channel = 'Z', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[5,18] ,y_lim=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<4','M>=6']\n",
    "plot_power([power_under_4_Z, power_events_mag69_Z], station = '', channel = 'Z', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[4,17] ,y_lim=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<3','M>=3','M>=4', 'M>=5', 'M>=6']\n",
    "plot_power(power_all_mags_Z, station = '', channel = 'Z', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[4,17] ,y_lim=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Canal E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_all_mags_E = [power_events_mag12_E, power_events_mag3_E, power_events_mag4_E, power_events_mag5_E, power_events_mag69_E]\n",
    "\n",
    "power_over_3_E = list(itertools.chain(power_events_mag3_E, power_events_mag4_E, power_events_mag5_E, power_events_mag69_E))\n",
    "\n",
    "power_over_4_E = list(itertools.chain(power_events_mag4_E, power_events_mag5_E, power_events_mag69_E))\n",
    "power_under_4_E = list(itertools.chain(power_events_mag12_E, power_events_mag3_E))\n",
    "\n",
    "power_over_5_E = list(itertools.chain(power_events_mag5_E, power_events_mag69_E))\n",
    "power_under_5_E = list(itertools.chain(power_under_4_E, power_events_mag4_E))\n",
    "\n",
    "power_under_6_E = list(itertools.chain(power_under_5_E, power_events_mag5_E))\n",
    "\n",
    "power_sep_3_E = [power_events_mag12_E, power_over_3_E]\n",
    "power_sep_4_E = [power_under_4_E, power_over_4_E]\n",
    "power_sep_5_E = [power_under_5_E, power_over_5_E]\n",
    "power_sep_6_E = [power_under_6_E, power_events_mag69_E]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<3','M>=3']\n",
    "plot_power(power_sep_3_E, station = '', channel = 'E' ,n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim= [20,35] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<4','M>=4']\n",
    "plot_power(power_sep_4_E, station = '', channel = 'E', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim= [20,35] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<5','M>=5']\n",
    "plot_power(power_sep_5_E, station = '', channel = 'E', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim= [20,35] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<6','M>=6']\n",
    "plot_power(power_sep_6_E, station = '', channel = 'E', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[20,35] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<3','M>=6']\n",
    "plot_power([power_events_mag12_E, power_events_mag69_E], station = '', channel = 'E', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[5,18] ,y_lim=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<4','M>=6']\n",
    "plot_power([power_under_4_E, power_events_mag69_E], station = '', channel = 'E' ,n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[20,35] ,y_lim=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<3','M>=3','M>=4', 'M>=5', 'M>=6']\n",
    "plot_power(power_all_mags_E, station = '', channel = 'E' ,n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[20,35] ,y_lim=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Canal N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_all_mags_N = [power_events_mag12_N, power_events_mag3_N, power_events_mag4_N, power_events_mag5_N, power_events_mag69_N]\n",
    "\n",
    "power_over_3_N = list(itertools.chain(power_events_mag3_N, power_events_mag4_N, power_events_mag5_N, power_events_mag69_N))\n",
    "\n",
    "power_over_4_N = list(itertools.chain(power_events_mag4_N, power_events_mag5_N, power_events_mag69_N))\n",
    "power_under_4_N = list(itertools.chain(power_events_mag12_N, power_events_mag3_N))\n",
    "\n",
    "power_over_5_N = list(itertools.chain(power_events_mag5_N, power_events_mag69_N))\n",
    "power_under_5_N = list(itertools.chain(power_under_4_N, power_events_mag4_N))\n",
    "\n",
    "power_under_6_N = list(itertools.chain(power_under_5_N, power_events_mag5_N))\n",
    "\n",
    "power_sep_3_N = [power_events_mag12_N, power_over_3_N]\n",
    "power_sep_4_N = [power_under_4_N, power_over_4_N]\n",
    "power_sep_5_N = [power_under_5_N, power_over_5_N]\n",
    "power_sep_6_N = [power_under_6_N, power_events_mag69_N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<3','M>=3']\n",
    "plot_power(power_sep_3_N, station = '', channel = 'N' ,n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim= [20,35] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<4','M>=4']\n",
    "plot_power(power_sep_4_N, station = '', channel = 'N', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim= [20,35] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<5','M>=5']\n",
    "plot_power(power_sep_5_N, station = '', channel = 'N', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim= [20,35] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<6','M>=6']\n",
    "plot_power(power_sep_6_N, station = '', channel = 'N', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[20,35] ,y_lim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<3','M>=6']\n",
    "plot_power([power_events_mag12_N, power_events_mag69_N], station = '', channel = 'N', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[5,18] ,y_lim=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<4','M>=6']\n",
    "plot_power([power_under_4_N, power_events_mag69_N], station = '', channel = 'N', n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[5,18] ,y_lim=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['M<3','M>=3','M>=4', 'M>=5', 'M>=6']\n",
    "plot_power(power_all_mags_N, station = '', channel = 'N' ,n_frames=10, use_log=True, height = 10, width = 7, event_type=event_type, x_lim=[20,35] ,y_lim=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Analisis de los eventos M>6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Aplicamos sta/lta a los eventos M>6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate sta/lta algo for the whole trace_mag69_v2_Z list\n",
    "trace_mag69_v2_Z = {k: v.trim(starttime=v.stats.starttime + 1, pad=True, fill_value=0) for k, v in trace_mag69_v2_Z.items()} # este código saca el primer segundo, esto ya que en el primer segundo hay como un pick extraño\n",
    "trace_mag69_v2_Z_stream = [Stream(traces = trace) for trace in trace_mag69_v2_Z.values()]\n",
    "sta_lta_mag6 = [p_picking_each(trace, ventana_10s = 10, ventana_30s = 30, nsta = 3, nlta = 13, thr_on = 4.2, thr_off = 1) for trace in trace_mag69_v2_Z_stream]\n",
    "\n",
    "ic(len(trace_mag69_v2_Z_stream))\n",
    "ic(len(sta_lta_mag6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analize_sta_lta(sta_lta: list, trace: dict, duration: float, off: bool = False):\n",
    "    tr = list(trace.values())[0]\n",
    "    key = list(trace.keys())[0]\n",
    "    if sta_lta is not None:\n",
    "        fig, ax = plt.subplots(figsize=(7, 3))\n",
    "        ax.plot(tr.times(), tr.data, 'k')\n",
    "        ax.set_title(f'Canal: {tr.stats.channel}. EventId: {key} ')\n",
    "        ax.set_xlabel('Time [s]')\n",
    "        ax.set_ylabel('Amplitude')\n",
    "        #ax.set_xlim(0, 100)\n",
    "        # add a vertical line where the cft array surpasses the threshold of 4.3\n",
    "        ax.axvline(sta_lta - tr.stats.starttime ,  color='r', linestyle='--', label='ON')\n",
    "        if off:\n",
    "            ax.axvline(sta_lta - tr.stats.starttime + duration ,  color='b', linestyle='--', label='OFF')\n",
    "        ax.legend()\n",
    "        #plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Se guardan como imagenes los eventos en la carpeta definida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las ID y data de los eventos\n",
    "output_folder = os.path.join('BD paper', 'catalogos', 'plots_señales_6')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "keys = list(trace_mag69_v2_Z.keys())\n",
    "values = list(trace_mag69_v2_Z.values())\n",
    "plt.rcParams['figure.constrained_layout.use'] = True\n",
    "\n",
    "sta_lta_mag6_clean = {}\n",
    "count= 0\n",
    "for i in range(len(sta_lta_mag6)):\n",
    "    key = keys[i]\n",
    "    value = values[i]\n",
    "    if sta_lta_mag6[i]:\n",
    "        count += 1\n",
    "        sta_lta_mag6_clean[key] = sta_lta_mag6[i][0]\n",
    "        # Extraer los datos y el tiempo del objeto de ObsPy\n",
    "        data = value.data\n",
    "        time = value.times('matplotlib')\n",
    "        \n",
    "        # Crear una figura y un eje con matplotlib\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))  # Cambiar el tamaño de la figura\n",
    "        \n",
    "        # Trazar los datos\n",
    "        ax.plot_date(time, data, '-')\n",
    "        \n",
    "        # Ajustar los ejes\n",
    "        ax.autoscale_view()\n",
    "        fig.autofmt_xdate()\n",
    "        #Agregar titulo\n",
    "        ax.set_title(f'Canal: {value.stats.channel}. EventId: {key} ')\n",
    "\n",
    "        # Agregar etiquetas a los ejes\n",
    "        ax.set_xlabel('Tiempo')\n",
    "        ax.set_ylabel('Amplitud')\n",
    "        \n",
    "        # Configurar las marcas del eje x para que aparezcan cada 30 segundos\n",
    "        ax.xaxis.set_major_locator(mdates.SecondLocator(interval=30))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "        \n",
    "        # Guardar la figura\n",
    "        plot_path = os.path.join(output_folder, f'evento_{key}.png')\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solo un ejemplo de evento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 9\n",
    "tr_mag69_list = list(trace_mag69_v2_Z.values())\n",
    "ic(sta_lta_mag6[idx])\n",
    "ic(tr_mag69_list[idx])\n",
    "tr_mag69_list[idx].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tiempos de incio quedaron guardados en el diccionario sta_lta_mag6_clean y en la lista. Esto en los casos donde las señales que son triggereadas por sta/lta. Hay 2 eventos que no fueron triggereados,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Cálculo del fin del evento.\n",
    "Esto se hace considerando cuando la energía baja del 3% de su peak por frames según estipulado en el paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ic(len(sta_lta_mag6_clean))\n",
    "ic(len(sta_lta_mag6))\n",
    "ic(len(trace_mag69_v2_Z))\n",
    "ic(len(trace_mag69_v2_Z_stream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se calcula el punto donde cada traza tendría su finalización del evento\n",
    "end_event = [endpoint_event(value.data, thr_energy = 0.97)[1] for value in values]\n",
    "peak_enegy = [endpoint_event(value.data)[0] for value in values]\n",
    "\n",
    "sliced_trs = []\n",
    "duration = []\n",
    "j = 0\n",
    "k = []\n",
    "for i in range(len(trace_mag69_v2_Z_stream)):\n",
    "    if sta_lta_mag6[i]:\n",
    "        sliced_trs.append(trace_mag69_v2_Z_stream[i].slice(starttime = sta_lta_mag6[i][0], endtime=sta_lta_mag6[i][0] + end_event[i]/40))\n",
    "        duration.append(sliced_trs[j][0].stats.endtime - sliced_trs[j][0].stats.starttime)\n",
    "        j += 1\n",
    "    else:\n",
    "        k.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se grafican las trazas cortadas\n",
    "sliced_trs[12].plot()\n",
    "print('Start time:', sliced_trs[12][0].stats.starttime)\n",
    "print('End time:', sliced_trs[12][0].stats.endtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Se plotean todos los eventos con sus tiempos de inicio y fin.\n",
    "Esta celda se debe correr, pero si no se quieren los gráficos, basta comentar la linea que tiene a la función analize_sta_lta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las ID y data de los eventos\n",
    "keys = list(trace_mag69_v2_Z.keys())\n",
    "values = list(trace_mag69_v2_Z.values())\n",
    "\n",
    "sta_lta_mag6_clean = {}\n",
    "count= 0\n",
    "j = 0\n",
    "for i in range(len(sta_lta_mag6)):\n",
    "    key = keys[i]\n",
    "    value = values[i]\n",
    "    if sta_lta_mag6[i]:\n",
    "        count += 1\n",
    "        sta_lta_mag6_clean[key] = sta_lta_mag6[i][0]\n",
    "        analize_sta_lta(sta_lta_mag6[i][0], {key: value}, duration[j], off=True)\n",
    "        j += 1\n",
    "\n",
    "# 4597528\n",
    "# 5159025\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decaimiento del 3% del frame con la energía máxima "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dur = np.array(duration)\n",
    "ic(np.mean(dur).round(2))\n",
    "# calcula la desviacion estandar de dur\n",
    "ic(np.std(dur).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_duration = os.path.join('BD paper', 'catalogos', 'duracion_eventos.xlsx')\n",
    "# take the column \"Duracion(s)\" and calculate the mean and std\n",
    "df_to_change = pd.read_excel(path_to_duration)\n",
    "\n",
    "# Calculate the mean and standard deviation\n",
    "mean_dur = np.mean(df_to_change['Duracion(s)']).round(2)\n",
    "std_dur = round(np.std(df_to_change['Duracion(s)']),2)\n",
    "max_dur = np.max(df_to_change['Duracion(s)'])\n",
    "min_dur = np.min(df_to_change['Duracion(s)'])\n",
    "\n",
    "# Add a new column 'Promedio' and 'Std' and set the first cell to the calculated values\n",
    "df_to_change.at[0, 'Promedio'] = mean_dur\n",
    "df_to_change.at[0, 'Sismo más largo'] = max_dur\n",
    "df_to_change.at[0, 'Sismo más corto'] = min_dur\n",
    "df_to_change.at[0, 'Std'] = std_dur\n",
    "\n",
    "# Save the modified DataFrame back to an Excel file\n",
    "df_to_change.to_excel(path_to_duration , index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Clasificación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['M<3', 'M>=6']\n",
    "\n",
    "power_all_mags_Z = [power_events_mag12_Z, power_events_mag3_Z, power_events_mag4_Z, power_events_mag5_Z, power_events_mag69_Z]\n",
    "\n",
    "\n",
    "#power_to_test_Z = [power_events_mag12_Z, power_events_mag5_Z ,power_events_mag69_Z]\n",
    "power_to_test_Z = [power_events_mag12_Z,power_events_mag69_Z]\n",
    "power_last_frame_Z = [arr[-1] for tup in power_to_test_Z  for arr in tup]\n",
    "\n",
    "X_z = np.array(power_last_frame_Z).reshape(-1,1)\n",
    "X_z = np.log10(X_z)\n",
    "\n",
    "y_z = np.concatenate([np.zeros(len(power_to_test_Z[0])),\n",
    "                         np.ones(len(power_to_test_Z[1]))])\n",
    "\n",
    "#power_to_test_E = [power_events_mag12_E, power_events_mag5_E ,power_events_mag69_E]\n",
    "power_to_test_E = [power_events_mag12_E,power_events_mag69_E]\n",
    "power_last_frame_E = [arr[-1] for tup in power_to_test_E  for arr in tup]\n",
    "\n",
    "X_e = np.array(power_last_frame_E).reshape(-1,1)\n",
    "X_e = np.log10(X_e)\n",
    "\n",
    "# y_e = np.concatenate([np.zeros(len(power_to_test_E[0])),\n",
    "#                          np.ones(len(power_to_test_E[1])),\n",
    "#                          np.ones(len(power_to_test_E[2]))])\n",
    "\n",
    "#power_to_test_N = [power_events_mag12_N, power_events_mag5_N ,power_events_mag69_N]\n",
    "power_to_test_N = [power_events_mag12_N,power_events_mag69_N]\n",
    "power_last_frame_N = [arr[-1] for tup in power_to_test_N  for arr in tup]\n",
    "\n",
    "X_n = np.array(power_last_frame_N).reshape(-1,1)\n",
    "X_n = np.log10(X_n)\n",
    "\n",
    "# y_n = np.concatenate([np.zeros(len(power_to_test_N[0])),\n",
    "#                          np.ones(len(power_to_test_N[1])),\n",
    "#                          np.ones(len(power_to_test_N[2]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lim = [5, 18]\n",
    "n_frames = 9\n",
    "means_z, std_z, covar_z, weights_z, pdf = plot_power([power_events_mag12_Z, power_events_mag69_Z], station = '', channel = 'Z',\n",
    "                                                       n_frames=n_frames, use_log=True, height = 10, width = 7, event_type=classes, x_lim=x_lim ,y_lim=45)\n",
    "\n",
    "means_e, std_e, covar_e, weights_e, pdf = plot_power([power_events_mag12_E, power_events_mag69_E], station = '', channel = 'E',\n",
    "                                                         n_frames=n_frames, use_log=True, height = 10, width = 7, event_type=classes, x_lim=x_lim ,y_lim=40)\n",
    "\n",
    "means_n, std_n, covar_n, weights_n, pdf = plot_power([power_events_mag12_N, power_events_mag69_N], station = '', channel = 'N',  \n",
    "                                                            n_frames=n_frames, use_log=True, height = 10, width = 7, event_type=classes, x_lim=x_lim ,y_lim=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_likelihood_classifier(x, class_1_mean, class_1_var, class_2_mean, class_2_var):\n",
    "    \"\"\"\n",
    "    Clasificador de Máxima Verosimilitud Simple.\n",
    "\n",
    "    Parameters:\n",
    "    - x: Valor a clasificar.\n",
    "    - class_1_mean: Media de la primera clase.\n",
    "    - class_1_var: Varianza de la primera clase.\n",
    "    - class_2_mean: Media de la segunda clase.\n",
    "    - class_2_var: Varianza de la segunda clase.\n",
    "\n",
    "    Returns:\n",
    "    - Clase asignada (1 o 2).\n",
    "    \"\"\"\n",
    "    prob_class_1 = 1 / np.sqrt(2 * np.pi * class_1_var) * np.exp(-(x - class_1_mean)**2 / (2 * class_1_var))\n",
    "    prob_class_2 = 1 / np.sqrt(2 * np.pi * class_2_var) * np.exp(-(x - class_2_mean)**2 / (2 * class_2_var))\n",
    "\n",
    "    if prob_class_1 > prob_class_2:\n",
    "        return 0  # Clase 1\n",
    "    else:\n",
    "        return 1  # Clase 2\n",
    "\n",
    "predicted_class_z = []\n",
    "for x in X_z:\n",
    "    predicted_class_z.append(max_likelihood_classifier(x, means_z[0], covar_z[0], means_z[1], covar_z[1]))\n",
    "\n",
    "predicted_class_z = np.array(predicted_class_z)\n",
    "cm_z = confusion_matrix(y_z, predicted_class_z)\n",
    "\n",
    "predicted_class_e = []\n",
    "for x in X_e:\n",
    "    predicted_class_e.append(max_likelihood_classifier(x, means_e[0], covar_e[0], means_e[1], covar_e[1]))\n",
    "\n",
    "cm_e = confusion_matrix(y_z, predicted_class_e)\n",
    "\n",
    "predicted_class_n = []\n",
    "for x in X_n:\n",
    "    predicted_class_n.append(max_likelihood_classifier(x, means_n[0], covar_n[0], means_n[1], covar_n[1]))\n",
    "\n",
    "cm_n = confusion_matrix(y_z, predicted_class_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un mapa de calor de la matriz de confusión para el canal Z, E y N\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 3), sharey=False, gridspec_kw={'wspace': 0.2})\n",
    "\n",
    "# Canal Z\n",
    "heatmap_z = sns.heatmap(cm_z, annot=True, fmt='d', cmap='Blues', linewidths=.5, cbar=False, ax=ax[0])\n",
    "heatmap_z.set_xticklabels(classes, rotation=45, ha='right', fontsize=12)\n",
    "heatmap_z.set_yticklabels(classes, rotation=0, fontsize=12)\n",
    "ax[0].set_title('Canal Z')\n",
    "ax[0].set_xlabel('Clase predicha')\n",
    "ax[0].set_ylabel('Clase verdadera')\n",
    "\n",
    "# Canal E\n",
    "heatmap_e = sns.heatmap(cm_e, annot=True, fmt='d', cmap='Blues', linewidths=.5, cbar=False, ax=ax[1])\n",
    "heatmap_e.set_xticklabels(classes, rotation=45, ha='right', fontsize=12)\n",
    "heatmap_e.set_yticklabels([])  # Eliminar etiquetas del eje y\n",
    "ax[1].set_title('Canal E')\n",
    "ax[1].set_xlabel('Clase predicha')\n",
    "ax[1].set_ylabel('Clase verdadera')\n",
    "\n",
    "# Canal N con barra de colores\n",
    "heatmap_n = sns.heatmap(cm_n, annot=True, fmt='d', cmap='Blues', linewidths=.5, cbar=False, ax=ax[2])\n",
    "heatmap_n.set_xticklabels(classes, rotation=45, ha='right', fontsize=12)\n",
    "heatmap_n.set_yticklabels([])  # Eliminar etiquetas del eje y\n",
    "ax[2].set_title('Canal N')\n",
    "ax[2].set_xlabel('Clase predicha')\n",
    "ax[2].set_ylabel('Clase verdadera')\n",
    "\n",
    "#fig.colorbar(ax[1].collections[0], cax=ax[3])\n",
    "\n",
    "# Ajustar el diseño para evitar superposiciones\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, vamos a combinar los datos de los canales Z y E en un solo array\n",
    "X_ze = np.concatenate((X_z, X_e), axis=1)\n",
    "\n",
    "# Ahora, vamos a calcular las medias y las matrices de covarianza para cada clase\n",
    "class_1_data = X_ze[y_z == 0]\n",
    "class_2_data = X_ze[y_z == 1]\n",
    "\n",
    "class_1_mean = np.mean(class_1_data, axis=0)\n",
    "class_2_mean = np.mean(class_2_data, axis=0)\n",
    "\n",
    "class_1_covar = np.cov(class_1_data, rowvar=False)\n",
    "class_2_covar = np.cov(class_2_data, rowvar=False)\n",
    "\n",
    "means_ze = [class_1_mean, class_2_mean]\n",
    "covar_ze = [class_1_covar, class_2_covar]\n",
    "\n",
    "# Ahora, vamos a definir un clasificador de máxima verosimilitud para dos dimensiones\n",
    "from scipy.stats import multivariate_normal\n",
    "def max_likelihood_classifier_2d(x, class_1_mean, class_1_covar, class_2_mean, class_2_covar):\n",
    "    \"\"\"\n",
    "    Clasificador de Máxima Verosimilitud para dos dimensiones.\n",
    "\n",
    "    Parameters:\n",
    "    - x: Valor a clasificar.\n",
    "    - class_1_mean: Media de la primera clase.\n",
    "    - class_1_covar: Matriz de covarianza de la primera clase.\n",
    "    - class_2_mean: Media de la segunda clase.\n",
    "    - class_2_covar: Matriz de covarianza de la segunda clase.\n",
    "\n",
    "    Returns:\n",
    "    - Clase asignada (1 o 2).\n",
    "    \"\"\"\n",
    "    prob_class_1 = multivariate_normal.pdf(x, mean=class_1_mean, cov=class_1_covar)\n",
    "    prob_class_2 = multivariate_normal.pdf(x, mean=class_2_mean, cov=class_2_covar)\n",
    "\n",
    "    if prob_class_1 > prob_class_2:\n",
    "        return 0  # Clase 1\n",
    "    else:\n",
    "        return 1  # Clase 2\n",
    "\n",
    "# Ahora, vamos a clasificar nuestros datos\n",
    "predicted_class_ze = []\n",
    "for x in X_ze:\n",
    "    predicted_class_ze.append(max_likelihood_classifier_2d(x, means_ze[0], covar_ze[0], means_ze[1], covar_ze[1]))\n",
    "\n",
    "cm_ze = confusion_matrix(y_z, predicted_class_ze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot matriz de confusion cm_ze \n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "heatmap_ze = sns.heatmap(cm_ze, annot=True, fmt='d', cmap='Blues', linewidths=.5, cbar=False)\n",
    "heatmap_ze.set_xticklabels(classes, rotation=45, ha='right', fontsize=12)\n",
    "heatmap_ze.set_yticklabels(classes, rotation=0, fontsize=12)\n",
    "# ax.set_title('Canales Z y E')\n",
    "ax.set_xlabel('Clase predicha')\n",
    "ax.set_ylabel('Clase verdadera')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Ellipse\n",
    "from matplotlib.patches import Ellipse\n",
    "# Creamos la figura\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "# Dibujamos los datos\n",
    "scatter = ax.scatter(X_ze[:, 0], X_ze[:, 1], c=y_z, cmap='viridis', alpha=0.8)\n",
    "# Calculamos las matrices de confusión\n",
    "cm_ze = confusion_matrix(y_z, predicted_class_ze)\n",
    "\n",
    "# Dibujamos los puntos mal clasificados en rojo\n",
    "misclassified = (y_z != predicted_class_ze)\n",
    "ax.scatter(X_ze[misclassified, 0], X_ze[misclassified, 1], c='red', label='Misclassified')\n",
    "\n",
    "# Calculamos las elipses para cada clase\n",
    "for i, color in enumerate(['blue', 'red']):\n",
    "    # Calculamos los vectores y valores propios de la matriz de covarianza\n",
    "    v, w = np.linalg.eigh(covar_ze[i])\n",
    "    v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "    u = w[0] / np.linalg.norm(w[0])\n",
    "\n",
    "    # Creamos la elipse\n",
    "    angle = np.arctan(u[1] / u[0])\n",
    "    angle = 180. * angle / np.pi  # convert to degrees\n",
    "    ell = Ellipse(means_ze[i], v[0], v[1], 180. + angle, color=color)\n",
    "    ell.set_clip_box(ax.bbox)\n",
    "    ell.set_alpha(0.2)\n",
    "    ax.add_artist(ell)\n",
    "\n",
    "# Añadimos las etiquetas\n",
    "plt.title('Clasificación de sismos')\n",
    "plt.xlabel('Canal Z')\n",
    "plt.ylabel('Canal E')\n",
    "plt.legend()\n",
    "\n",
    "# Mostramos el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confussion matrix cm_ze \n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "heatmap_ze = sns.heatmap(cm_ze, annot=True, fmt='d', cmap='Blues', linewidths=.5, cbar=False)\n",
    "heatmap_ze.set_xticklabels(classes, rotation=45, ha='right', fontsize=12)\n",
    "heatmap_ze.set_yticklabels(classes, rotation=0, fontsize=12)\n",
    "ax.set_title('Canales Z y E')\n",
    "ax.set_xlabel('Clase predicha')\n",
    "ax.set_ylabel('Clase verdadera')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARA 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, vamos a combinar los datos de los canales Z, E y N en un solo array\n",
    "X_zen = np.concatenate((X_z, X_e, X_n), axis=1)\n",
    "\n",
    "# Ahora, vamos a calcular las medias y las matrices de covarianza para cada clase\n",
    "class_1_data = X_zen[y_z == 0]\n",
    "class_2_data = X_zen[y_z == 1]\n",
    "\n",
    "class_1_mean = np.mean(class_1_data, axis=0)\n",
    "class_2_mean = np.mean(class_2_data, axis=0)\n",
    "\n",
    "class_1_covar = np.cov(class_1_data, rowvar=False)\n",
    "class_2_covar = np.cov(class_2_data, rowvar=False)\n",
    "\n",
    "means_zen = [class_1_mean, class_2_mean]\n",
    "covar_zen = [class_1_covar, class_2_covar]\n",
    "\n",
    "#matriz de confusion\n",
    "cm_zen = confusion_matrix(y_z, predicted_class_zen)\n",
    "\n",
    "# Ahora, vamos a definir un clasificador de máxima verosimilitud para tres dimensiones\n",
    "def max_likelihood_classifier_3d(x, class_1_mean, class_1_covar, class_2_mean, class_2_covar):\n",
    "    \"\"\"\n",
    "    Clasificador de Máxima Verosimilitud para tres dimensiones.\n",
    "\n",
    "    Entradas:\n",
    "    - x: Valor a clasificar.\n",
    "    - class_1_mean: Media de la primera clase.\n",
    "    - class_1_covar: Matriz de covarianza de la primera clase.\n",
    "    - class_2_mean: Media de la segunda clase.\n",
    "    - class_2_covar: Matriz de covarianza de la segunda clase.\n",
    "\n",
    "    Salida:\n",
    "    - Clase asignada (0 o 1).\n",
    "    \"\"\"\n",
    "    log_prob_class_1 = multivariate_normal.logpdf(x, mean=class_1_mean, cov=class_1_covar)\n",
    "    log_prob_class_2 = multivariate_normal.logpdf(x, mean=class_2_mean, cov=class_2_covar)\n",
    "\n",
    "    # Calculamos la \"distancia\" como la diferencia de log-verosimilitudes\n",
    "    distance = log_prob_class_1 - log_prob_class_2\n",
    "\n",
    "    if distance > 0:\n",
    "        return 0, distance  # Clase 1\n",
    "    else:\n",
    "        return 1, distance  # Clase 2\n",
    "\n",
    "# Ahora, vamos a clasificar nuestros datos\n",
    "predicted_class_zen = []\n",
    "distances = []\n",
    "for x in X_zen:\n",
    "    predicted_class, distance = max_likelihood_classifier_3d(x, means_zen[0], covar_zen[0], means_zen[1], covar_zen[1])\n",
    "    predicted_class_zen.append(predicted_class)\n",
    "    if predicted_class == 0:\n",
    "        distances.append(distance)\n",
    "    distances.append(distance)\n",
    "\n",
    "plt.hist(distances, bins=30)\n",
    "plt.title('Histograma de distancias')\n",
    "plt.xlabel('Distancia')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y finalmente, vamos a visualizar los resultados\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(17, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_zen[:, 0], X_zen[:, 1], X_zen[:, 2], c=predicted_class_zen, cmap='viridis')\n",
    "ax.set_title('Clasificación de sismos')\n",
    "ax.set_xlabel('Canal Z')\n",
    "ax.set_ylabel('Canal E')\n",
    "ax.set_zlabel('Canal N')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la figura\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Dibujamos los datos\n",
    "scatter = ax.scatter(X_zen[:, 0], X_zen[:, 1], X_zen[:, 2], c=y_z, cmap='viridis', alpha=0.8)\n",
    "\n",
    "# Dibujamos los puntos mal clasificados en rojo\n",
    "misclassified = (y_z != predicted_class_zen)\n",
    "ax.scatter(X_zen[misclassified, 0], X_zen[misclassified, 1], X_zen[misclassified, 2], c='red', label='Misclassified')\n",
    "\n",
    "# Añadimos las etiquetas\n",
    "ax.set_title('Clasificación de sismos')\n",
    "ax.set_xlabel('Canal Z')\n",
    "ax.set_ylabel('Canal E')\n",
    "ax.set_zlabel('Canal N')\n",
    "plt.legend()\n",
    "\n",
    "# Mostramos el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Función para dibujar un elipsoide\n",
    "def plot_cov_ellipse(cov, pos, nstd=2, ax=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Plota un elipsoide de confianza basado en una matriz de covarianza dada y una posición media.\n",
    "    \"\"\"\n",
    "    def eigsorted(cov):\n",
    "        vals, vecs = np.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]\n",
    "        return vals[order], vecs[:,order]\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    vals, vecs = eigsorted(cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "\n",
    "    # Ancho y altura son escalas de sigma principal\n",
    "    width, height, z_height = 2 * nstd * np.sqrt(vals)\n",
    "    ellip = Ellipse(xy=pos, width=width, height=height, angle=theta, **kwargs)\n",
    "\n",
    "    ax.add_artist(ellip)\n",
    "    return ellip\n",
    "\n",
    "# Dibujamos los datos con los elipsoides\n",
    "fig = plt.figure(figsize=(17, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_zen[:, 0], X_zen[:, 1], X_zen[:, 2], c=predicted_class_zen, cmap='viridis')\n",
    "ax.set_title('Clasificación de sismos')\n",
    "ax.set_xlabel('Canal Z')\n",
    "ax.set_ylabel('Canal E')\n",
    "ax.set_zlabel('Canal N')\n",
    "\n",
    "# Dibujamos los elipsoides para cada clase\n",
    "plot_cov_ellipse(covar_zen[0], means_zen[0], ax=ax, alpha=0.2, color='blue')\n",
    "plot_cov_ellipse(covar_zen[1], means_zen[1], ax=ax, alpha=0.2, color='red')\n",
    "\n",
    "# Identificamos los puntos mal clasificados\n",
    "predicted_class_zen = np.array(predicted_class_zen)\n",
    "misclassified = predicted_class_zen != y_z\n",
    "ax.scatter(X_zen[misclassified, 0], X_zen[misclassified, 1], X_zen[misclassified, 2], c='red', marker='x')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confussion matrix cm_zen\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "heatmap_zen = sns.heatmap(cm_zen, annot=True, fmt='d', cmap='Blues', linewidths=.5, cbar=False)\n",
    "heatmap_zen.set_xticklabels(classes, rotation=45, ha='right', fontsize=12)\n",
    "heatmap_zen.set_yticklabels(classes, rotation=0, fontsize=12)\n",
    "ax.set_title('Canales Z, E y N')\n",
    "ax.set_xlabel('Clase predicha')\n",
    "ax.set_ylabel('Clase verdadera')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Gaussian Mixtures Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['M<3', 'M>=5']\n",
    "\n",
    "power_all_mags_Z = [power_events_mag12_Z, power_events_mag3_Z, power_events_mag4_Z, power_events_mag5_Z, power_events_mag69_Z]\n",
    "\n",
    "\n",
    "power_to_test_Z = [power_events_mag12_Z, power_events_mag5_Z ,power_events_mag69_Z]\n",
    "power_last_frame_Z = [arr[-1] for tup in power_to_test_Z  for arr in tup]\n",
    "\n",
    "X_z = np.array(power_last_frame_Z).reshape(-1,1)\n",
    "X_z = np.log10(X_z)\n",
    "\n",
    "y_z = np.concatenate([np.zeros(len(power_to_test_Z[0])),\n",
    "                         np.ones(len(power_to_test_Z[1])),\n",
    "                         np.ones(len(power_to_test_Z[2]))])\n",
    "\n",
    "power_to_test_E = [power_events_mag12_E, power_events_mag5_E ,power_events_mag69_E]\n",
    "power_last_frame_E = [arr[-1] for tup in power_to_test_E  for arr in tup]\n",
    "\n",
    "X_e = np.array(power_last_frame_E).reshape(-1,1)\n",
    "X_e = np.log10(X_e)\n",
    "\n",
    "y_e = np.concatenate([np.zeros(len(power_to_test_E[0])),\n",
    "                         np.ones(len(power_to_test_E[1])),\n",
    "                         np.ones(len(power_to_test_E[2]))])\n",
    "\n",
    "power_to_test_N = [power_events_mag12_N, power_events_mag5_N ,power_events_mag69_N]\n",
    "power_last_frame_N = [arr[-1] for tup in power_to_test_N  for arr in tup]\n",
    "\n",
    "X_n = np.array(power_last_frame_N).reshape(-1,1)\n",
    "X_n = np.log10(X_n)\n",
    "\n",
    "y_n = np.concatenate([np.zeros(len(power_to_test_N[0])),\n",
    "                         np.ones(len(power_to_test_N[1])),\n",
    "                         np.ones(len(power_to_test_N[2]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Primero para el canal Z\n",
    "X_train_Z, X_test_Z, y_train_Z, y_test_Z = train_test_split(X_z, y_z, test_size=0.3, random_state=rng)\n",
    "X_train_E, X_test_E, y_train_E, y_test_E = train_test_split(X_e, y_e, test_size=0.3, random_state=rng)\n",
    "X_train_N, X_test_N, y_train_N, y_test_N = train_test_split(X_n, y_n, test_size=0.3, random_state=rng)\n",
    "\n",
    "# Crear un modelo GMM para cada canal\n",
    "gmm_z = GaussianMixture(n_components=2, random_state=rng)\n",
    "gmm_e = GaussianMixture(n_components=2, random_state=rng)\n",
    "gmm_n = GaussianMixture(n_components=2, random_state=rng)\n",
    "\n",
    "# Entrenar los modelos\n",
    "gmm_z.fit(X_train_Z)\n",
    "gmm_e.fit(X_train_E)\n",
    "gmm_n.fit(X_train_N)\n",
    "\n",
    "# Predecir las etiquetas para los datos\n",
    "labels_z = gmm_z.predict(X_test_Z)\n",
    "labels_e = gmm_e.predict(X_test_E)\n",
    "labels_n = gmm_n.predict(X_test_N)\n",
    "\n",
    "# Ordenar las etiquetas según la clase con más elementos\n",
    "if np.sum(labels_z == 0) > np.sum(labels_z == 1):\n",
    "    labels_z = 1 - labels_z\n",
    "\n",
    "if np.sum(labels_e == 0) > np.sum(labels_e == 1):\n",
    "    labels_e = 1 - labels_e\n",
    "\n",
    "if np.sum(labels_n == 0) > np.sum(labels_n == 1):\n",
    "    labels_n = 1 - labels_n\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "cm_Z = confusion_matrix(y_test_Z, labels_z)\n",
    "cm_E = confusion_matrix(y_test_E, labels_e)\n",
    "cm_N = confusion_matrix(y_test_N, labels_n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un mapa de calor de la matriz de confusión para el canal Z, E y N\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 3), sharey=False, gridspec_kw={'wspace': 0.2})\n",
    "\n",
    "# Canal Z\n",
    "heatmap_z = sns.heatmap(cm_Z, annot=True, fmt='d', cmap='Blues', linewidths=.5, cbar=False, ax=ax[0])\n",
    "heatmap_z.set_xticklabels(classes, rotation=45, ha='right', fontsize=12)\n",
    "heatmap_z.set_yticklabels(classes, rotation=0, fontsize=12)\n",
    "ax[0].set_title('Canal Z')\n",
    "ax[0].set_xlabel('Clase predicha')\n",
    "ax[0].set_ylabel('Clase verdadera')\n",
    "\n",
    "# Canal E\n",
    "heatmap_e = sns.heatmap(cm_E, annot=True, fmt='d', cmap='Blues', linewidths=.5, cbar=False, ax=ax[1])\n",
    "heatmap_e.set_xticklabels(classes, rotation=45, ha='right', fontsize=12)\n",
    "heatmap_e.set_yticklabels([])  # Eliminar etiquetas del eje y\n",
    "ax[1].set_title('Canal E')\n",
    "ax[1].set_xlabel('Clase predicha')\n",
    "ax[1].set_ylabel('Clase verdadera')\n",
    "\n",
    "# Canal N con barra de colores\n",
    "heatmap_n = sns.heatmap(cm_N, annot=True, fmt='d', cmap='Blues', linewidths=.5, cbar=False, ax=ax[2])\n",
    "heatmap_n.set_xticklabels(classes, rotation=45, ha='right', fontsize=12)\n",
    "heatmap_n.set_yticklabels([])  # Eliminar etiquetas del eje y\n",
    "ax[2].set_title('Canal N')\n",
    "ax[2].set_xlabel('Clase predicha')\n",
    "ax[2].set_ylabel('Clase verdadera')\n",
    "\n",
    "#fig.colorbar(ax[1].collections[0], cax=ax[3])\n",
    "\n",
    "# Ajustar el diseño para evitar superposiciones\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canal Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_z = X_test_Z.squeeze()\n",
    "\n",
    "f=X_test_z.reshape(-1,1)\n",
    "\n",
    "weights_z = gmm_z.weights_\n",
    "means_z = gmm_z.means_\n",
    "covars_z = gmm_z.covariances_\n",
    "\n",
    "\n",
    "\n",
    "x_axis = X_test_z\n",
    "x_axis.sort()\n",
    "print('Weights:', weights_z)\n",
    "print('Means:', means_z.squeeze())\n",
    "print('Covars:', covars_z)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 4))\n",
    "\n",
    "ax.hist(f, bins=50, histtype='bar', density=True, ec='red', alpha=0.5)\n",
    "ax.plot(x_axis,weights_z[0]*stats.norm.pdf(x_axis,means_z[0],np.sqrt(covars_z[0])).ravel(), c='red')\n",
    "ax.plot(x_axis,weights_z[1]*stats.norm.pdf(x_axis,means_z[1],np.sqrt(covars_z[1])).ravel(), c='green')\n",
    "ax.title.set_text('Distribución de la potencia para el canal Z')\n",
    "\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(20, 4))\n",
    "\n",
    "# Individual bars for each data point\n",
    "for i, x_val in enumerate(X_test_z):\n",
    "    ax.bar(x_val, 1, color='blue' if labels_z[i] == 0 else 'green', alpha=0.2, width=0.05)\n",
    "\n",
    "# Probability density curves\n",
    "ax.plot(x_axis, weights_z[0] * stats.norm.pdf(x_axis, means_z[0], np.sqrt(covars_z[0])).ravel(), c='blue')\n",
    "ax.plot(x_axis, weights_z[1] * stats.norm.pdf(x_axis, means_z[1], np.sqrt(covars_z[1])).ravel(), c='green')\n",
    "\n",
    "# Title and grid\n",
    "ax.set_title('Distribution of power for channel Z')\n",
    "ax.grid()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_e = X_e.squeeze()\n",
    "\n",
    "f=X_e.reshape(-1,1)\n",
    "\n",
    "weights_e = gmm_e.weights_\n",
    "means_e = gmm_e.means_\n",
    "covars_e = gmm_e.covariances_\n",
    "\n",
    "\n",
    "\n",
    "x_axis = X_e\n",
    "x_axis.sort()\n",
    "print('Weights:', weights_e)\n",
    "print('Means:', means_e.squeeze())\n",
    "print('Covars:', covars_e)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 4))\n",
    "\n",
    "ax.hist(f, bins=100, histtype='bar', density=True, ec='red', alpha=0.5)\n",
    "ax.plot(x_axis,weights_e[0]*stats.norm.pdf(x_axis,means_e[0],np.sqrt(covars_e[0])).ravel(), c='red')\n",
    "ax.plot(x_axis,weights_e[1]*stats.norm.pdf(x_axis,means_e[1],np.sqrt(covars_e[1])).ravel(), c='green')\n",
    "ax.title.set_text('Distribución de la potencia para el canal E')\n",
    "\n",
    "ax.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(20, 4))\n",
    "\n",
    "# Individual bars for each data point\n",
    "for i, x_val in enumerate(X_test_E):\n",
    "    ax.bar(x_val, 1, color='blue' if labels_e[i] == 0 else 'green', alpha=0.2, width=0.05)\n",
    "    if labels_e[i]!=y_test_E[i]:\n",
    "        print('Error en el evento:', i)\n",
    "        print('Etiqueta predicha:', labels_e[i])\n",
    "        print('Etiqueta verdadera:', y_test_E[i])\n",
    "        print('Potencia:', X_test_E[i])\n",
    "\n",
    "\n",
    "# Probability density curves\n",
    "ax.plot(x_axis, weights_e[0] * stats.norm.pdf(x_axis, means_e[0], np.sqrt(covars_e[0])).ravel(), c='green')\n",
    "ax.plot(x_axis, weights_e[1] * stats.norm.pdf(x_axis, means_e[1], np.sqrt(covars_e[1])).ravel(), c='blue')\n",
    "\n",
    "# Title and grid\n",
    "ax.set_title('Distribution of power for channel E')\n",
    "ax.grid()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_e == y_test_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_n = X_n.squeeze()\n",
    "\n",
    "f=X_n.reshape(-1,1)\n",
    "\n",
    "weights_n = gmm_n.weights_\n",
    "means_n = gmm_n.means_\n",
    "covars_n = gmm_n.covariances_\n",
    "\n",
    "\n",
    "\n",
    "x_axis = X_n\n",
    "x_axis.sort()\n",
    "print('Weights:', weights_n)\n",
    "print('Means:', means_n.squeeze())\n",
    "print('Covars:', covars_n)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 4))\n",
    "\n",
    "ax.hist(f, bins=100, histtype='bar', density=True, ec='red', alpha=0.5)\n",
    "ax.plot(x_axis,weights_n[0]*stats.norm.pdf(x_axis,means_n[0],np.sqrt(covars_n[0])).ravel(), c='red')\n",
    "ax.plot(x_axis,weights_n[1]*stats.norm.pdf(x_axis,means_n[1],np.sqrt(covars_n[1])).ravel(), c='green')\n",
    "ax.title.set_text('Distribución de la potencia para el canal N')\n",
    "\n",
    "ax.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:, ::-1] # flip axes for better plotting\n",
    "\n",
    "\n",
    "X_stretched = np.dot(X, rng.randn(2, 2))\n",
    "\n",
    "\n",
    "from matplotlib.patches import Ellipse\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height, \n",
    "                             angle, **kwargs))\n",
    "        \n",
    "        \n",
    "def plot_gmm(gmm, X, label=True, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=7, cmap='viridis', zorder=2)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=7, zorder=2)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, alpha=w * w_factor)       \n",
    "\n",
    "\n",
    "plot_gmm(gmm, X_stretched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trigger_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
