{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "# %matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from itertools import product\n",
    "from itertools import groupby\n",
    "import itertools\n",
    "import pickle\n",
    "import glob\n",
    "import chardet\n",
    "import copy\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Set a professional style for the plot\n",
    "plt.style.use('_mpl-gallery')\n",
    "import matplotlib.dates as mdaates\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.cm as cm\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset, plot_trigger\n",
    "from obspy import Trace\n",
    "from obspy.imaging.spectrogram import spectrogram\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from tqdm.auto import tqdm\n",
    "from icecream import ic\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm, kstest, skew, kurtosis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Local Imports\n",
    "from energy.utils_general import *\n",
    "from energy.utils_energy import *\n",
    "from energy.preprocessing import *\n",
    "from energy.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tener en consideración. Las rutas a los archivos están guardadas de la siguiente forma: *waveform_M/event_ID/channel/station/archivos.mseed*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creamos un diccionario con las coordenadas de las estaciones del litoral a partir del archivo Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_P = 8.064\n",
    "\n",
    "file_stations_coord = \"Estaciones_Litoral.xlsx\"\n",
    "df = pd.read_excel(file_stations_coord)\n",
    "stations_coord_all = df.set_index('Estacion')[['Lat', 'Lon']].apply(tuple, axis=1).to_dict()\n",
    "stations_names = list(stations_coord_all.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Trabajar con la base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. La función z_channel_dict entrega un nested dictionary donde la primera capa de llaves indica la magnitud, la segunda capa el evento, la tercera la red de la estación que la captó, y finalmente el valor es la traza ya leída por obspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to all the folders from the folder BD paper that is in the current directory\n",
    "folders = glob.glob('BD paper/*')\n",
    "# Get all the folders in the BD paper folder that start with waveform \n",
    "folders_signals = [folder for folder in folders if folder.startswith('BD paper/waveform')]\n",
    "folders_signals = sorted(folders_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BD paper/waveform_4p1_0.5',\n",
       " 'BD paper/waveform_4p2_0.5',\n",
       " 'BD paper/waveform_5_0.5',\n",
       " 'BD paper/waveform_6-9_0.5']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders_signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función se puede/debe optimizar. Usando os.walk probablemente sale más rápido. No sé si existe alguna forma de hacerla más generalizable eso si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_channel_dict(folders_signals):\n",
    "\n",
    "\n",
    "    result_dict = {}\n",
    "\n",
    "    for folder in folders_signals:\n",
    "        waveform_type = os.path.split(folder)[-1]\n",
    "        events_id = glob.glob(os.path.join(folder, '*'))\n",
    "        #ic(waveform_type, events_id)\n",
    "\n",
    "        for event_id in events_id:\n",
    "            events_name = os.path.split(event_id)[-1]\n",
    "            network_path = glob.glob(os.path.join(event_id, '*'))\n",
    "\n",
    "            for network_folder in network_path:\n",
    "                network_name = os.path.split(network_folder)[-1]\n",
    "                \n",
    "                stations_path = glob.glob(os.path.join(network_folder, '*'))\n",
    "                #ic(stations_path)\n",
    "\n",
    "                for station_path in stations_path:\n",
    "                    stations_names = os.path.split(station_path)[-1]\n",
    "                    stations_path = glob.glob(os.path.join(station_path, '*'))\n",
    "\n",
    "                    stations_ch_z = [station_z for station_z in stations_path if 'BHZ' in station_z]\n",
    "\n",
    "                    # Read and store traces using ObsPy\n",
    "                    traces = []\n",
    "                    for station_z_path in stations_ch_z:\n",
    "                        trace = read(station_z_path)[0]  # Assuming one trace per file\n",
    "                        traces.append(trace)\n",
    "\n",
    "\n",
    "                    result_dict.setdefault(waveform_type, {}).setdefault(events_name, {}).setdefault(network_name, {}).setdefault(stations_names, traces)\n",
    "\n",
    "    return result_dict\n",
    "result_dict = z_channel_dict(folders_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_type = list(result_dict.keys())\n",
    "\n",
    "# get all the events ids consideting waveform type is a list. And make events_ids a dictionary with the waveform type as keys\n",
    "events_ids = {waveform_type: [list(result_dict[waveform_type].keys())]for waveform_type in waveform_type}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. La siguiente función pone a todas las traces de TODOS los eventos en una fila, esto lo tengo a priori, quizás habrá que arreglarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_traces(nested_dict):\n",
    "    traces = []\n",
    "    for value in nested_dict.values():\n",
    "        if isinstance(value, dict):\n",
    "            traces.extend(get_all_traces(value))\n",
    "        else:\n",
    "            traces.extend(value)\n",
    "    return traces\n",
    "\n",
    "# Get all ObsPy Trace objects in the dictionary\n",
    "all_traces = get_all_traces(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BD paper/catalogos',\n",
       " 'BD paper/waveform_6-9_0.5',\n",
       " 'BD paper/waveform_4p1_0.5',\n",
       " 'BD paper/waveform_4p2_0.5',\n",
       " 'BD paper/waveform_5_0.5']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 El siguiente código es para crear un nuevo archivo CSV que contenga solo los siguientes datos:\n",
    "- EventId\n",
    "- Time\n",
    "- Latitud\n",
    "- Longitud\n",
    "- Magnitud\n",
    "- Estación (la que aparece automáticamente en el catálogo)\n",
    "- Estación más cercana (calculada con geodisic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_coordinates(files_to_add, files_with_coord):\n",
    "    \n",
    "    for file_to_add, file_with_coords in zip(files_to_add, files_with_coord):\n",
    "        df_to_add = pd.read_csv(file_to_add, sep=',')\n",
    "        df_with_coords = pd.read_csv(file_with_coords, sep='|')\n",
    "\n",
    "        df_to_add['Time'] = pd.to_datetime(df_to_add['Time'])\n",
    "        df_with_coords['Time'] = pd.to_datetime(df_with_coords['Time'])\n",
    "\n",
    "        df_to_add = df_to_add.set_index('Time')\n",
    "\n",
    "        # Merge df_to_add with df_with_coords on 'Time', keeping only 'Latitude' and 'Longitude' from df_with_coords\n",
    "        df_to_add = pd.merge(df_to_add, df_with_coords[['Time','Latitude', 'Longitude']], on='Time', how='left', suffixes=('', '_y'))\n",
    "\n",
    "        # Drop the '_y' columnsbb\n",
    "        df_to_add = df_to_add.drop(columns=['Latitude_y', 'Longitude_y'], errors='ignore')\n",
    "\n",
    "        df_to_add.to_csv(file_to_add, index=False, sep = ',')\n",
    "\n",
    "\n",
    "# Get all the csv files in the BD paper folder that have the word \"cercanos\" in their name\n",
    "files_raw = sorted(glob.glob(os.path.join('BD paper', 'catalogos','*cercanos*.csv')))\n",
    "# Get all the csv files in the BD paper folder that have the word \"full_data\" in their name\n",
    "files_full_data = sorted(glob.glob(os.path.join('BD paper', 'lista de datos','*full_data*.csv')))\n",
    "\n",
    "\n",
    "# Call the function to merge coordinates for each pair of files\n",
    "merge_coordinates(files_raw, files_full_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BD paper/catalogos/Eventos_descargados_4p1.csv',\n",
       " 'BD paper/catalogos/Eventos_descargados_4p2.csv',\n",
       " 'BD paper/catalogos/Eventos_descargados_5.csv']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the csv files in the BD paper folder that have the word \"cercanos\" in their name\n",
    "files_raw = sorted(glob.glob(os.path.join('BD paper', 'catalogos','*.csv')))\n",
    "\n",
    "files_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Lee el archivo Excel original\n",
    "df_original = pd.read_excel(\"tu_archivo_original.xlsx\")\n",
    "\n",
    "# Diccionario de eventos\n",
    "events_ids = {\n",
    "    'waveform_4p1_0.5': [['5190277', '10071115', '5178072', '11019653', '11090343', '11340398', '5185702', '10896756']],\n",
    "    'waveform_4p2_0.5': [['4597464', '5174164', '11137931', '11207110', '10000295', '11272932', '11204853', '4600957', '5188856', '11304668', '11252658']]\n",
    "}\n",
    "\n",
    "# Filtrar y seleccionar columnas\n",
    "filtered_df = pd.DataFrame()\n",
    "for waveform_type, event_ids_list in events_ids.items():\n",
    "    for event_ids in event_ids_list:\n",
    "        filtered_df = filtered_df.append(df_original[df_original['#EventID'].astype(str).isin(event_ids)])\n",
    "\n",
    "# Seleccionar solo las columnas requeridas\n",
    "filtered_df = filtered_df[['#EventID', 'Time', 'Latitude', 'Longitude', 'Magnitude', 'Estacion']]\n",
    "\n",
    "# Guardar el nuevo DataFrame en un nuevo archivo Excel\n",
    "filtered_df.to_excel(\"nuevo_archivo.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Ahora que ya se tienen archivos .csv con columnas \"Time\", \"Magnitud\", \"Estación\", \"Latitud\", \"Longitud\", podemos verificar efectivamente cuál es la estación más cercana y agregarla como última columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BD paper/lista de datos/Eventos_500_cercanos_4p1.csv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = files_raw[0]\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Leer el archivo CSV\n",
    "with open(file_path, 'r') as f:\n",
    "    df_events = pd.read_csv(f)\n",
    "\n",
    "# Cambiar nombre de la columna \"Time\" a \"Fecha UTC\" y cosas en inglés por español\n",
    "df_events = df_events.rename(columns={'Time': 'Fecha UTC'})\n",
    "# Cambiar esta columna a formato UTC\n",
    "df_events['Fecha UTC'] = pd.to_datetime(df_events['Fecha UTC'])\n",
    "df_events = df_events.rename(columns={'Latitude': 'Latitud'})\n",
    "df_events = df_events.rename(columns={'Longitude': 'Longitud'})\n",
    "\n",
    "# Escribir en el archivo CSV\n",
    "with open(file_path, 'w') as f:\n",
    "    df_events.to_csv(f, index=False, sep=',')\n",
    "\n",
    "# tomar el valor maximo y minimo de la magnitud en el dataframe\n",
    "max_magnitude = df_events['Magnitud'].max()\n",
    "min_magnitude = df_events['Magnitud'].min()\n",
    "\n",
    "df_new = calculate_detection_times(df_events , stations_coord_all, v_P, magnitude_range = (min_magnitude, max_magnitude))\n",
    "_, closest_sts_names = nearest_n_stations(df_new, stations_names, 1)\n",
    "\n",
    "# Leer el archivo CSV\n",
    "with open(file_path, 'r') as f:\n",
    "    df_events = pd.read_csv(f)\n",
    "\n",
    "\n",
    "df_events['Estación más cercana'] = closest_sts_names[0]\n",
    "df_events.to_csv(file_path, index=False, sep = ',')\n",
    "# Escribir en el archivo CSV\n",
    "with open(file_path, 'w') as f:\n",
    "    df_events.to_csv(f, index=False, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def closest_station(file_path, stations_coord_all, v_P, n_closest_stations = 5):\n",
    "    '''\n",
    "    Actualiza todos los archivos csv con la estación más cercana a cada evento sísmico.\n",
    "    '''\n",
    "\n",
    "    for file_path in files_raw:\n",
    "\n",
    "        # Leer el archivo CSV\n",
    "        df_events = pd.read_csv(file_path)\n",
    "\n",
    "        # Cambiar nombre de la columna \"Time\" a \"Fecha UTC\" y cosas en inglés por español\n",
    "        df_events = df_events.rename(columns={'Time': 'Fecha UTC'})\n",
    "        \n",
    "        # Cambiar esta columna a formato UTC\n",
    "        df_events['Fecha UTC'] = pd.to_datetime(df_events['Fecha UTC'])\n",
    "        df_events = df_events.rename(columns={'Latitude': 'Latitud', 'Longitude': 'Longitud'})\n",
    "\n",
    "        # Escribir en el archivo CSV\n",
    "        df_events.to_csv(file_path, index=False, sep=',')\n",
    "\n",
    "        # Tomar el valor máximo y mínimo de la magnitud en el dataframe\n",
    "        max_magnitude = df_events['Magnitud'].max()\n",
    "        min_magnitude = df_events['Magnitud'].min()\n",
    "\n",
    "        df_new = calculate_detection_times(df_events, stations_coord_all, v_P, magnitude_range=(min_magnitude, max_magnitude))\n",
    "        _, closest_sts_names = nearest_n_stations(df_new, stations_names, n_closest_stations)\n",
    "\n",
    "        # Leer el archivo CSV\n",
    "        df_events = pd.read_csv(file_path)\n",
    "\n",
    "        # Agregar columnas para cada estación cercana\n",
    "        for i in range(n_closest_stations):\n",
    "            col_name = f'Estación más cercana {i+1}'\n",
    "            df_events[col_name] = closest_sts_names[i]\n",
    "\n",
    "        # Escribir en el archivo CSV\n",
    "        df_events.to_csv(file_path, index=False, sep=',')\n",
    "\n",
    "\n",
    "closest_station(files_raw, stations_coord_all, v_P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trigger_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
